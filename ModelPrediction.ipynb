{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47e6a144",
   "metadata": {},
   "source": [
    "# **Insert Title Here**\n",
    "In this notebook, you can try out the different models trained in order to detect if the text inputted is suicidal or not.\n",
    "\n",
    "**DATA103 S11 Group 4**\n",
    "- GOZON, Jean Pauline D.\n",
    "- JAMIAS, Gillian Nicole A.\n",
    "- MARCELO Andrea Jean C. \n",
    "- REYES, Anton Gabriel G.\n",
    "- VICENTE, Francheska Josefa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ad502",
   "metadata": {},
   "source": [
    "## Requirements and Imports\n",
    "\n",
    "Before starting, the relevant libraries and files in building and training the model should be loaded into the notebook first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fb834c",
   "metadata": {},
   "source": [
    "**Basic Libraries**\n",
    "\n",
    "Import `numpy`, `pandas`, and `datasets`.\n",
    "\n",
    "* `numpy` contains a large collection of mathematical functions\n",
    "* `pandas` contains functions that are designed for data manipulation and data analysis\n",
    "* `datasets` contains functions that allow easier pre-processing for datasets and smart caching for easier loading of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3142be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d178a08f",
   "metadata": {},
   "source": [
    "**Natural Language Processing Libraries** \n",
    "\n",
    "The next imports are libraries that can implement feature engineering techniques on the text input. \n",
    "* `re` is a module that allows the use of regular expressions\n",
    "* `TFidfVectorizer` converts the given text documents into a matrix, which has TF-IDF features\n",
    "* `CountVectorizer` converts the given text documents into a matrix, which has the counts of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb5b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4576c25",
   "metadata": {},
   "source": [
    "**Machine Learning Libraries**\n",
    "\n",
    "`pickle` is a module that can serialize and deserialize objects. In this notebook, it is used to save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6898cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e068fc4",
   "metadata": {},
   "source": [
    "The following classes are classifiers that implement different methods of classification.\n",
    "* `LogisticRegression` is a class under the linear models module that implements regularized logistic regression\n",
    "* `MultinomialNB` is a class under the Naive Bayes module that allows the classification of discrete features\n",
    "* `RandomForestClassifier` is a class under the ensemble module that trains by fitting using a number of decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c869ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f51e72",
   "metadata": {},
   "source": [
    "Last, `requests` is a library that allows us to send requests to the Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20508fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b79e3a6",
   "metadata": {},
   "source": [
    "## Model Imports\n",
    "To use the model, let us import each of the model that we used, starting with the vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0625a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('./saved_models/trad_ml/vectorizers/count.pkl', \"rb\") as file:\n",
    "    count_vectorizer = pickle.load(file)\n",
    "    \n",
    "with open ('./saved_models/trad_ml/vectorizers/tfidf.pkl', \"rb\") as file:\n",
    "    tfidf_vectorizer = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e7d5fb",
   "metadata": {},
   "source": [
    "Before we move on to importing the models, let us first declare the list that would hold the models that we will be using."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ce9a7",
   "metadata": {},
   "source": [
    "Now, we can continue with importing the models (that utilized traditional machine learning algorithms) that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e695050c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_models = []\n",
    "tfidf_models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['logreg', 'logreg_tuned', 'mnb', 'mnb_tuned']\n",
    "\n",
    "for temp in file_names:\n",
    "    with open ('./saved_models/trad_ml/' + temp + '/count/model.pkl', \"rb\") as file:\n",
    "        model = pickle.load(file)\n",
    "        count_models.append(model)\n",
    "        \n",
    "    with open ('./saved_models/trad_ml/' + temp + '/tfidf/model.pkl', \"rb\") as file:\n",
    "        model = pickle.load(file)\n",
    "        tfidf_models.append(model)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fbb78f",
   "metadata": {},
   "source": [
    "Next, let us declare the URL and the header that will allow us to send requests to the BERT and the RoBERTa models from the Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b374bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROBERTA_API_URL = \"https://api-inference.huggingface.co/models/francheska-vicente/data103-roberta-base-v1\"\n",
    "roberta_headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "BERT_API_URL = \"https://api-inference.huggingface.co/models/francheska-vicente/data103-bert-base-v2\"\n",
    "bert_headers = {\"Authorization\": \"Bearer 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\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad972d1",
   "metadata": {},
   "source": [
    "Last, we will be defining the function that will send the request to the Hub and accept the response of the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dce8093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(payload, url, header):\n",
    "    response = requests.post(url, headers=header, json=payload)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb27adcd",
   "metadata": {},
   "source": [
    "## Declaration of Functions \n",
    "\n",
    "Before we start with the prediction proper, we will have to define the functions that will be used.\n",
    "\n",
    "First, the `remove_unnecessary` function will remove the words that we deem as unnecessary (i.e., retweets, usernames, media links, square brackets, and hashtags)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unnecessary(text):\n",
    "    text = re.sub('RT', '', text) # RT\n",
    "    text = re.sub('@[^\\s]+', '', text) # usernames\n",
    "    text = re.sub('http[^\\s]+','',text) # media links\n",
    "    text = re.sub(r'\\[|\\]', '', text) # square brackets\n",
    "    text = re.sub('#[^ ]+', '', text) # hashtags\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d7a943",
   "metadata": {},
   "source": [
    "Next, the `clean_input` function is the function that will call all functions that will be used for pre-processing and cleaning. This will be used for the models that utilized traditional machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021056ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input (text):\n",
    "    text = remove_unnecessary (text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3862304",
   "metadata": {},
   "source": [
    "This is followed by the functions that will be used for formatting the output of the predictions. The `determine_output` is the function that will convert the response of the Huggingface Hub into the same format as the output of traditional machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c3dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_output (output):\n",
    "    negative = output[0][0]['score']\n",
    "    positive = output[0][1]['score']\n",
    "    \n",
    "    label = 0\n",
    "    probability = negative \n",
    "    \n",
    "    if positive >= 0.5:\n",
    "        label = 1\n",
    "        probability = positive\n",
    "        \n",
    "    return [label, probability]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2326c32",
   "metadata": {},
   "source": [
    "Last, the `output_probabilities` function will format the output of the models to make it more understandable and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc736a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_probabilities (predictions, probabilities):\n",
    "    labels = ['Logistic Regression', 'Tuned Logistic Regression',\n",
    "             'Multinomial Naive Bayes', 'Tuned Multinomial Naive Bayes',\n",
    "             'BERT', 'RoBERTa']\n",
    "\n",
    "    for i in range (len(labels)):\n",
    "        predict_label = 'non-suicidal'\n",
    "        \n",
    "        curr_probability = probabilities [i][0]\n",
    "        probability_label = round(curr_probability [0] * 100, 2) \n",
    "        if (predictions [i] == 1):\n",
    "            predict_label = 'suicidal'\n",
    "            probability_label = round(curr_probability [1] * 100, 2)  \n",
    "        \n",
    "        print ('According to the ' + labels [i] + ' model, there is a ' + str(probability_label) + \n",
    "               '% chance that your text is ' + predict_label + '.')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69479cd8",
   "metadata": {},
   "source": [
    "## Try out our model!\n",
    "Now, you can use our models! Enter **STOP** if you want to stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9724236",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input ('Enter text: ').strip ()\n",
    "\n",
    "while (text.lower() != 'stop'):\n",
    "    count_text = count_vectorizer.transform ([clean_input (text)])\n",
    "    tfidf_text = tfidf_vectorizer.transform ([clean_input (text)])\n",
    "    \n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    \n",
    "    for curr_model in count_models:\n",
    "        predictions.append(curr_model.predict(count_text))\n",
    "        probabilities.append(curr_model.predict_proba(count_text))\n",
    "        \n",
    "    for curr_model in tfidf_models:\n",
    "        predictions.append(curr_model.predict(tfidf_text))\n",
    "        probabilities.append(curr_model.predict_proba(tfidf_text))\n",
    "    \n",
    "    try:\n",
    "        bert_output = determine_output(query(text, BERT_API_URL, bert_headers))\n",
    "        roberta_output = determine_output(query(text, ROBERTA_API_URL, roberta_headers))\n",
    "    \n",
    "        predictions.append(bert_output [0])\n",
    "        probabilities.append(bert_output [1])\n",
    "    \n",
    "        predictions.append(roberta_output [0])\n",
    "        probabilities.append(roberta_output [1])\n",
    "    except:\n",
    "        print(\"The BERT and/or RoBERTa models are still being loaded to the Huggingface Hub.\")\n",
    "    \n",
    "    output_probabilities (predictions, probabilities)\n",
    "    \n",
    "    text = input ('Enter text: ').strip ()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
