{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQpzOBMe7UKh"
   },
   "source": [
    "# **Insert Title Here**\n",
    "**DATA103 S11 Group 4**\n",
    "- GOZON, Jean Pauline D.\n",
    "- JAMIAS, Gillian Nicole A.\n",
    "- MARCELO Andrea Jean C. \n",
    "- REYES, Anton Gabriel G.\n",
    "- VICENTE, Francheska Josefa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iaqOqa27AJa"
   },
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MX7Oyiuw7u76"
   },
   "source": [
    "### **Requirements and Imports**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLhPdUtW7zkB"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "McPgWjwsglQw"
   },
   "source": [
    "**Basic Libraries**\n",
    "\n",
    "* `numpy` contains a large collection of mathematical functions\n",
    "* `pandas` contains functions that are designed for data manipulation and data analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "82rNPPaTpyfe"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjRvGoC9g_25"
   },
   "source": [
    "**Visualization Libraries**\n",
    "\n",
    "* `matplotlib.pyplot` contains functions to create interactive plots\n",
    "* `seaborn` is a library based on matplotlib that allows for data visualization\n",
    "* `spacy` is a Python-based open-source library used in processing text data. \n",
    "* `wordcloud` contains functions for generating wordclouds from text data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YGgFzaZp1MO"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import ImageColorGenerator\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUGR_tXwiDhV"
   },
   "source": [
    "**Natural Language Processing Libraries**\n",
    "* `re` is a module that allows the use of regular expressions\n",
    "* `nltk` provides functions for processing text data\n",
    "* `stopwords` is a corpus from NLTK, which includes a compiled list of stopwords\n",
    "* `Counter` is from Python's collections module, which is helpful for tokenization\n",
    "* `string` contains functions for string operations\n",
    "* `TFidfVectorizer` converts the given text documents into a matrix, which has TF-IDF features\n",
    "* `CountVectorizer` converts the given text documents into a matrix, which has the counts of the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ka0AZNlQp_xU",
    "outputId": "50dc49c4-e664-4b42-b7e6-0ece77f9a357"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import ngrams\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_M-Pj_A4Ona"
   },
   "source": [
    "**Machine Learning Libraries**\n",
    "\n",
    "* `torch` this is an open source ML library for deep neural network creation\n",
    "* `transformers` contains pre-trained models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yIVc5EA6GNZC"
   },
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vS_U_vK24OEf"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertModel \n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "riB5SpFCpWPj"
   },
   "source": [
    "**Google Drive**\n",
    "* `google.colab` a library that allows the colab notebook to mount the google drive"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ISceA7sdpasq",
    "outputId": "cf693468-3103-4748-dc33-48ac1304e12c"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zBoR7vo738e"
   },
   "source": [
    "#### Datasets and Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKKTacy6lVth"
   },
   "source": [
    "The following `.csv` files were used in this project:\n",
    "* `Suicide_Detection.csv` contains the text itself and the two classes namely suicide and non-suicide. Retrieved from the \"Suicide and Depression Detection\" in Kaggle\n",
    "* `twitter-suicidal-intention-dataset.csv` similar to `Suicide_Detection.csv` but intention is numbered. For the intention column, 1 means tweet is suicidal and 0 means it is not. Retrieved from github.\n",
    "* `500_anonymized_Reddit_users_posts_labels.csv` contains text from a post and the label of intention. (4 labels available)\n",
    "* `suicide notes.csv` contains text of suicide notes but it does not have a column labelling the notes as suicidal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-XfJf6p7-Pc"
   },
   "source": [
    "## **Data Collection**\n",
    "\n",
    "```\n",
    "# This is formatted as code\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k68uY6RKpMVE",
    "outputId": "b22aa29f-da4a-43b9-9d6d-d7acde1b97e3"
   },
   "outputs": [],
   "source": [
    "#importing the .csv file from kaggle\n",
    "watch_df = pd.read_csv('data/Suicide_Detection.csv')\n",
    "watch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z5wQ0AK5tuDx",
    "outputId": "d5d9d128-c85b-4ef3-e185-7e012a40185e"
   },
   "outputs": [],
   "source": [
    "print(watch_df[\"class\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34i0ZI6Epa44",
    "outputId": "1c334fb2-8e07-4577-e684-24cbcac1b196"
   },
   "outputs": [],
   "source": [
    "# importing the twitter dataset\n",
    "url = \"https://raw.githubusercontent.com/laxmimerit/twitter-suicidal-intention-dataset/master/twitter-suicidal_data.csv\"\n",
    "twit_df = pd.read_csv(url)\n",
    "twit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifyiMD6Ltyak",
    "outputId": "60a51cba-e000-4964-94ab-fec9f1bdc7b1"
   },
   "outputs": [],
   "source": [
    "print(twit_df[\"intention\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlYQI6R9xBN_",
    "outputId": "f08f2ec5-5d8c-4c74-f5f0-75c6310caea6"
   },
   "outputs": [],
   "source": [
    "anon_df = pd.read_csv(\"data/500_anonymized_Reddit_posts.csv\")\n",
    "anon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48-weQ2ltI1E",
    "outputId": "92742ead-41d8-4cc2-c8ef-74630b8cd62e"
   },
   "outputs": [],
   "source": [
    "print(anon_df[\"Label\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6huuvsTLxdUU",
    "outputId": "edf0467f-3be3-4bf8-cb48-66b455f50e2e"
   },
   "outputs": [],
   "source": [
    "notes_df = pd.read_csv(\"data/suicide notes.csv\")\n",
    "notes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zYEi6d5ooAV"
   },
   "source": [
    "## **Description of the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9oC3Sfrtrjse",
    "outputId": "6c4d14b1-8d31-46f6-9638-e29ade0a179a"
   },
   "outputs": [],
   "source": [
    "#getting the shape of the four datasets\n",
    "display(watch_df.shape, twit_df.shape, anon_df.shape, notes_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spPo2VPAvQW5",
    "outputId": "3ed17013-b363-41ae-f1b8-e5ee6cde9766"
   },
   "outputs": [],
   "source": [
    "print(watch_df.info(), twit_df.info(), anon_df.info(), notes_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZcBY0G80dQe"
   },
   "source": [
    "After seeing the number of features in each column per dataset, dataframes `watch_df`, `twit_df`, and `anon_df` are complete. However, dataframe `notes_df` contains null values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcjZ3AyQ8NzR"
   },
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4PGobu8vBNU"
   },
   "source": [
    "### **Pre-Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SqgyWNIRGXe"
   },
   "source": [
    "#### DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXMEc7FMlauJ"
   },
   "source": [
    "Since the dataframe `notes_df` has null values, we will get rid of those rows using panda's dropna() function. Setting the axis to 0 allows us to drop rows which contain missing values. Additionally, the how parameter set to any causes that row to be removed if there is at least 1 null value present in that row. Having inplace equal to true modifies the exisiting Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oxrNve17WhB"
   },
   "outputs": [],
   "source": [
    "notes_df.dropna(axis = 0, how = \"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BANqANG6mD81"
   },
   "source": [
    "After checking the total number of null values  in the whole `notes_df` we can see it is equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D1DpVQ46mEq2",
    "outputId": "e2a9219f-1628-4040-c3ca-1c3041c66b69"
   },
   "outputs": [],
   "source": [
    "notes_df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0BbdXYD2YE8"
   },
   "source": [
    "For some of the dataframes, the `user`, `id`, and `unnamed column` would not be needed and therefore would be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MlLQibBwvEoP"
   },
   "outputs": [],
   "source": [
    "anon_df = anon_df.drop(\"User\", axis = 1)\n",
    "notes_df = notes_df.drop(\"id\", axis = 1)\n",
    "watch_df = watch_df.drop(\"Unnamed: 0\", axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6AoHzbmi3Sth",
    "outputId": "69dc9729-bb64-49d4-8d20-681259cd4590"
   },
   "outputs": [],
   "source": [
    "display(\"anon_df\",anon_df.columns, \"notes_df\", notes_df.columns, \"watch_df\", watch_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUExBkes3Vyy",
    "outputId": "b6b03afd-36c1-4677-e4ac-7f2e248d6cb5"
   },
   "outputs": [],
   "source": [
    "display(\"anon_df\",anon_df.head(), \n",
    "        \"notes_df\", notes_df.head(), \n",
    "        \"twit_df\", twit_df.head(), \n",
    "        \"watch_df\", watch_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEteXaw5575a"
   },
   "source": [
    "After dropping the unecessary columns, it was then time to convert the values for the labeling columns so that once the dataframes are joined, there wouldn't be any further complications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz6XTH0Sxv-v"
   },
   "source": [
    "Reviewing the columns of all the datasets we imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24nVPlF7pkte",
    "outputId": "da508950-e40d-418e-ed0b-1250c4ceb42d"
   },
   "outputs": [],
   "source": [
    "display(\"anon_df columns\", list(anon_df.columns),\"notes_df columns\", list(notes_df.columns),\n",
    "        \"twit_df columns\", list(twit_df.columns), \"watch_df columns\", list(watch_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vk9bQq_Mx2R_"
   },
   "source": [
    "Creating a copy of watch_df before modifying the values to match twit_df (1 means text is suicidal and 0 means it is not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjNn_mhru_Qo"
   },
   "outputs": [],
   "source": [
    "integerwatch_df = watch_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "strshyWRyG7w"
   },
   "source": [
    "Using pandas replace() function to change multiple values with multiple new values for an individual DataFrame column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pz6Xni0VwPgm"
   },
   "outputs": [],
   "source": [
    "integerwatch_df['class'] = integerwatch_df['class'].replace(['suicide', 'non-suicide'], ['1', '0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9h_52AmxpGP",
    "outputId": "f93fe024-cc84-4c7c-df77-8c70897114ab"
   },
   "outputs": [],
   "source": [
    " integerwatch_df.head() #checking if the replace function reflected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4PvgNbANuNa"
   },
   "source": [
    "Using .info() to check the datatypes of the dataframe intgerwatch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgfC9cVZNgwH",
    "outputId": "618c22a3-6540-4efe-ea03-88dbce53f0bf"
   },
   "outputs": [],
   "source": [
    " integerwatch_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xA5UEOJRM6AN"
   },
   "source": [
    "Using panads astype() function allows us to convert the obj data type in the class column to integer for uniformity with other dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nefy2j7xMl7O"
   },
   "outputs": [],
   "source": [
    "integerwatch_df['class'] = integerwatch_df['class'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNXLdiPb0By5"
   },
   "source": [
    "Creating a copy of notes_df before modifying the values to match twit_df (1 means text is suicidal and 0 means it is not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z-fob8XGN6iA",
    "outputId": "b7847b1f-78fb-4a9b-ea68-34f07d0e6387"
   },
   "outputs": [],
   "source": [
    " integerwatch_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgN7pqi_yXQw"
   },
   "outputs": [],
   "source": [
    "integernotes_df = notes_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lwj0k6XC0QWM"
   },
   "source": [
    "Creating a new column named class and setting it to have a constant value of 1 since all texts are posted by users with  suicidal thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0tKdd5Vz2pZ"
   },
   "outputs": [],
   "source": [
    "integernotes_df['class'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZtTBxop2zhR2",
    "outputId": "bd05802b-a4d5-4f40-e303-0e05265144ef"
   },
   "outputs": [],
   "source": [
    "integernotes_df.head() #checking if the new class column was addded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFGd439I07ab"
   },
   "source": [
    "Creating a copy of twit_df before modifying the column names to match integerwatch_df and integernotes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5Jdx3Yy0r7Y"
   },
   "outputs": [],
   "source": [
    "new_twit = twit_df.copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRqBHPkJ154a"
   },
   "source": [
    "Renaming using pandas rename() function using a dictionary of new and old column names. Inplace set to true modified the existing Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lgrNL_H1etD"
   },
   "outputs": [],
   "source": [
    "new_twit.rename(columns={\"tweet\": \"text\", \"intention\": \"class\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9PqaVotI1GFW",
    "outputId": "4280e4e4-aaff-43e1-e190-2f16b9c01eec"
   },
   "outputs": [],
   "source": [
    "new_twit.head() #checking if column names are renamed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u55d_8IqI3Vz"
   },
   "source": [
    "For `anon_df`, there are five unique values with their respective counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PFvIJx-EIiEH",
    "outputId": "2ba2da4a-fb5a-4f80-a159-1fd86ad641b8"
   },
   "outputs": [],
   "source": [
    "anon_df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DlavMLJTe0R"
   },
   "source": [
    "Columns for `anon_df` were renamed for consistency with the other dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0VjftZc6MbUG"
   },
   "outputs": [],
   "source": [
    "anon_df.rename(columns={\"Post\": \"text\", \"Label\": \"class\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wj4NkFZjTq_U"
   },
   "source": [
    "Copying `anon_df` before modifying other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKLm371oM1Qj"
   },
   "outputs": [],
   "source": [
    "intanon_df = anon_df.copy(deep =  True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pccnf7P_TrpW"
   },
   "source": [
    "Using pandas `replace()` function to change multiple values with multiple new values for an individual DataFrame column. For `intanon_df` the 5 values were replaced with a corrosponding `1` or `0` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VV_rBqZJFKW"
   },
   "outputs": [],
   "source": [
    "intanon_df['class'] = intanon_df['class'].replace(['Ideation', 'Indicator','Behavior','Attempt','Supportive'], ['1','1','1','1','0'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2boS4R1TvFW"
   },
   "source": [
    "Using the `.astype()` function to convert the `class` value's types into `int` or an `integer` type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubvRMUTztqk1"
   },
   "outputs": [],
   "source": [
    "intanon_df['class'] = intanon_df['class'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92wWCQn8TveC"
   },
   "source": [
    "Looking at the tail of `intanon_df` to check if the `class` values were replaced and converted accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2W4-fz02OLt4",
    "outputId": "d2af4b2a-c25e-4c1b-8b74-f81315fa82e3"
   },
   "outputs": [],
   "source": [
    "intanon_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hED83js6OQ01"
   },
   "source": [
    "After changing column names for `anon_df` the following values were changed:\n",
    "\n",
    "* Ideation = 1\n",
    "* Indicatior = 1\n",
    "* Behavior = 1\n",
    "* Attempt = 1\n",
    "* Supportive = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgaWLGzkRPwh"
   },
   "source": [
    "#### All dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYbNvGV73s0p"
   },
   "source": [
    "Displaying the dataframes we have now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBhfWOt13427",
    "outputId": "4f923ca9-9245-4878-c627-a8936e1b94e1"
   },
   "outputs": [],
   "source": [
    "display(\"intanon_df\",intanon_df.head(), \n",
    "        \"integernotes_df\", integernotes_df.head(), \n",
    "        \"new_twit\", new_twit.head(), \n",
    "        \"integerwatch_df\", integerwatch_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZRgJiUFwJzkV",
    "outputId": "db80912b-a63e-4a12-9339-19a9b22ae66e"
   },
   "outputs": [],
   "source": [
    "#getting the shape of the four datasets \n",
    "display(intanon_df.shape, integernotes_df.shape, new_twit.shape, integerwatch_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H3rST7vDOadS"
   },
   "source": [
    "Using the concat() function with an axis set to 0 allows us to stitch Dataframes along the rows. We will first combine the Dataframes integernotes_df and new_twit to one Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7uLGJODHfgQ"
   },
   "outputs": [],
   "source": [
    "concat = pd.concat([intanon_df, integernotes_df, new_twit, integerwatch_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MCHQEJrPY06"
   },
   "source": [
    "We can check if the number of rows are equal to the total of the two combined Dataframes. 9586 rows is the sum of 467 rows and 9119 rows.\n",
    "\n",
    "As well as checking if the unique values are still integers 1 and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JYJxwLbKYlH",
    "outputId": "b74a7c6c-a30f-4fb5-d2fa-17225914f140"
   },
   "outputs": [],
   "source": [
    "display(concat.shape , concat[\"class\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8eWSSaoUBM4"
   },
   "source": [
    "We switch the placement of the `text` and `class` columns in all four dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikPoyZ1gU65n"
   },
   "outputs": [],
   "source": [
    "new_column_order = ['class', 'text'] #making the new column order\n",
    "\n",
    "intanon_df = intanon_df[new_column_order]\n",
    "integernotes_df = integernotes_df[new_column_order]\n",
    "new_twit = new_twit[new_column_order]\n",
    "integerwatch_df = integerwatch_df[new_column_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ys69wdN8U44"
   },
   "source": [
    "### **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dBDZg8I8KF6v"
   },
   "source": [
    "#### Removing unnecessary character sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKdZuo_c2Uma"
   },
   "source": [
    "We created a RegEx function to remove unnecessary character sequences that might potentially interfere with the next steps before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRFHwCvdvn5e"
   },
   "outputs": [],
   "source": [
    "def remove_unnecessary(text):\n",
    "    text = re.sub('RT', '', text) #RT\n",
    "    text = re.sub('@[^\\s]+', '', text) #usernames\n",
    "    text = re.sub('http[^\\s]+','',text) #media links\n",
    "    text = re.sub(r'\\[|\\]', '', text) #square brackets\n",
    "    text = re.sub('#[^ ]+', '', text) #hashtags\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofSseOGf2pQw"
   },
   "source": [
    "But before applying the function, a copy of the `concat` dataframe was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sAA4cNeq2oSu"
   },
   "outputs": [],
   "source": [
    "master = concat.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chbEe4ST3eXd"
   },
   "source": [
    "Here, the function was applied to the `master` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZYfoY_z22To"
   },
   "outputs": [],
   "source": [
    "master['text'] = master['text'].apply(remove_unnecessary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcTjvzecJ5ZQ"
   },
   "source": [
    "Checking if the `remove_unnecessary` function was applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJnermyqJ2pg",
    "outputId": "a11b3371-d6c3-4f75-d218-e5d979c752c4"
   },
   "outputs": [],
   "source": [
    "master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y7wVvGEvJ_LF"
   },
   "source": [
    "Checking the shape of the `master` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zXZYoEGurSJY",
    "outputId": "94316941-8ebd-4a30-91c7-15c0cad92b10"
   },
   "outputs": [],
   "source": [
    "master.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6nFnPeTmtLu"
   },
   "source": [
    "#### Functions for Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LDM36VBnc_3"
   },
   "source": [
    "**Batch Processing Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdwLR7W-nlMv"
   },
   "source": [
    "Because the `master` dataframe is big, the four dataframes will be processed and cleaned by batch with a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6azFtS_IKPqD"
   },
   "outputs": [],
   "source": [
    "def batch_processing_bert(df):\n",
    "    \n",
    "    def tokenize_and_remove_stopwords(sentence):\n",
    "        tokens = tokenizer.tokenize(sentence)\n",
    "        return tokens\n",
    "\n",
    "    df['token'] = df['text'].apply(tokenize_and_remove_stopwords)\n",
    "    df['string'] = df['token'].apply(lambda x: ' '.join([item for item in x if len(item)>2]))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kVyp7urc2ES"
   },
   "source": [
    "**Removing UNK Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Tt22Hh3fYqO"
   },
   "outputs": [],
   "source": [
    "unk_pattern = re.compile(r'\\bUNK\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7fwTnPOaFuZW"
   },
   "outputs": [],
   "source": [
    "def remove_UNK(text):\n",
    "    return re.sub(r\"\\bUNK\\b\", \"\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QyaqcG2w8XDK"
   },
   "source": [
    "### **Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fasJUJOHCsCd"
   },
   "source": [
    "#### **Tokenizing with Bert**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QmznKkIHSFI"
   },
   "source": [
    "We get the tokenizer for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctjQyQHxEgps"
   },
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_wordpiece=True)\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCwsGjvha9BS"
   },
   "source": [
    "Making copies of all the four dataframes for easier access without affecting the original dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHkJMrJOMCwr"
   },
   "outputs": [],
   "source": [
    "bert_df1 = intanon_df.copy(deep = True)\n",
    "bert_df2 = integernotes_df.copy(deep = True)\n",
    "bert_df3 = new_twit.copy(deep = True)\n",
    "bert_df4 = integerwatch_df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t235o6XUb4hU"
   },
   "source": [
    "Merging all the copied dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HjJJtCwck8P"
   },
   "outputs": [],
   "source": [
    "bert_concat = pd.concat([bert_df1, bert_df2, bert_df3, bert_df4], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "icKf4qklcfXj"
   },
   "outputs": [],
   "source": [
    "bert_text_data = bert_concat['text'].tolist()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "bert_tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_data = bert_tokenizer(bert_text_data, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Add the tokenized data as columns to the DataFrame\n",
    "token_columns = ['token_{}'.format(i) for i in range(tokenized_data.input_ids.shape[1])]\n",
    "df_tokens = pd.DataFrame(tokenized_data.input_ids.numpy(), columns=token_columns)\n",
    "bert_master = pd.concat([bert_concat, df_tokens], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TNiND5HhSXL"
   },
   "source": [
    "##### **Tokenized Text**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gweFATQ7hZ_b"
   },
   "source": [
    "  Creating a list of the copied dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrq7U7ZdLVo2"
   },
   "outputs": [],
   "source": [
    "bert_dflist = [bert_df1, bert_df2, bert_df3, bert_df4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-Vy4EZsHKG6"
   },
   "source": [
    "Looping through the list to batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UG9IBYAGHJ4A",
    "outputId": "e1a9be9b-db40-4b65-d7cb-57f47f00273f"
   },
   "outputs": [],
   "source": [
    "for df in bert_dflist:\n",
    "    df = batch_processing_bert(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pX4UNCR_atHo"
   },
   "source": [
    "After the loop, we display the first 5 rows of all the dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dSA_88fL5wL"
   },
   "outputs": [],
   "source": [
    "display(bert_df1.head(), bert_df2.head(), bert_df3.head(), bert_df4.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_q3ESUicmA1"
   },
   "source": [
    "Merging all the looped dataframes into one master dataframe.\n",
    "\n",
    "After that, we make a copy of the dataframe and use the `remove_unnecessary` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h0az2Anjcb5E"
   },
   "outputs": [],
   "source": [
    "bert_concat_text = pd.concat([bert_df1, bert_df2, bert_df3, bert_df4], axis=0)\n",
    "\n",
    "bert_master_text = bert_concat_text.copy(deep = True)\n",
    "\n",
    "bert_master_text['string'] = bert_master_text['string'].apply(remove_unnecessary)\n",
    "\n",
    "bert_master_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U83pYVOnC2HJ"
   },
   "source": [
    "#### **Tokenizing with NLTK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPWY4KoiXsD8"
   },
   "source": [
    "Creating copies and concatenating the copied dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFDq2V1KMmpe"
   },
   "outputs": [],
   "source": [
    "nltk_df1 = intanon_df.copy(deep = True)\n",
    "nltk_df2 = integernotes_df.copy(deep = True)\n",
    "nltk_df3 = new_twit.copy(deep = True)\n",
    "nltk_df4 = integerwatch_df.copy(deep = True)\n",
    "\n",
    "nltk_concat = pd.concat([nltk_df1, nltk_df2, nltk_df3, nltk_df4], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uz0K-JP9X3mu"
   },
   "source": [
    "We get the `RegexpTokenizer` by creating a tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAJOcD1xNYsp"
   },
   "outputs": [],
   "source": [
    "nltk_concat['text'] = nltk_concat['text'].astype(str).str.lower()\n",
    "regexp = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOl-yaE2YlEh"
   },
   "source": [
    "We create a new column in the `nltk_concat` dataframe to apply the tokenized text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9opKs2CNQ4j"
   },
   "outputs": [],
   "source": [
    "nltk_concat['text_token']=nltk_concat['text'].apply(regexp.tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlf76bYuZem6"
   },
   "source": [
    "Creating a copy of the `nltk_concat` dataframe and renaming it to be consistent with the other tokenized dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TCItAQlOlm7"
   },
   "outputs": [],
   "source": [
    "nltk_master = nltk_concat.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKUgu7SuZnrZ"
   },
   "source": [
    "We display the head of the dataframe to see the result of the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xK4k-9Y1ODHl",
    "outputId": "7e2d6a30-ad21-4a9f-df90-3368bb35d605"
   },
   "outputs": [],
   "source": [
    "nltk_master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OwBLpdAwCsUM"
   },
   "source": [
    "#### **Tokenizing with TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZipIFo_hZw-T"
   },
   "source": [
    "Creating copies and concatenating the copied dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIx4SoSEJal9"
   },
   "outputs": [],
   "source": [
    "tfidf_df1 = intanon_df.copy(deep = True)\n",
    "tfidf_df2 = integernotes_df.copy(deep = True)\n",
    "tfidf_df3 = new_twit.copy(deep = True)\n",
    "tfidf_df4 = integerwatch_df.copy(deep = True)\n",
    "\n",
    "tfidf_concat = pd.concat([tfidf_df1, tfidf_df2, tfidf_df3, tfidf_df4], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhPY0E5eZ8oc"
   },
   "source": [
    "We extract the text data into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gm3t19EMNr5t"
   },
   "outputs": [],
   "source": [
    "tfidf_text_data = tfidf_concat['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHi5Lnw1aBDj"
   },
   "source": [
    "We then create a `TfidfVectorizer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sgkOn_BWNhcI"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JML5IupRaHK0"
   },
   "source": [
    "We fit the `tfidf_vectorizer` onto the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "5Q4qDBX3Nopa",
    "outputId": "d88544de-efdc-491d-a2e2-f3c32092d6d7"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer.fit(tfidf_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4FnBUXtaRhk"
   },
   "source": [
    "We transform the text data into the TF_IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tf7MIszTNogE"
   },
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.transform(tfidf_text_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW4hIK44aWDA"
   },
   "source": [
    "Lastly, we add the TF-IDF matrix as columns in the `tfidf_master` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-lIQ1z7SPU2"
   },
   "outputs": [],
   "source": [
    "tfidf_columns = ['tfidf_{}'.format(i) for i in range(tfidf_matrix.shape[1])]\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_columns)\n",
    "tfidf_master = pd.concat([tfidf_concat, df_tfidf], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpLYX3Mtaf4V"
   },
   "source": [
    "We display the head of the dataframe to see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "abw4uM5KOyQQ"
   },
   "outputs": [],
   "source": [
    "tfidf_master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTPJ1W4ICsl6"
   },
   "source": [
    "#### **Tokenizing with CountVectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UupFR1VSbF1N"
   },
   "source": [
    "Creating copies and concatenating the copied dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_Wv42ZgUoIc"
   },
   "outputs": [],
   "source": [
    "count_df1 = intanon_df.copy(deep = True)\n",
    "count_df2 = integernotes_df.copy(deep = True)\n",
    "count_df3 = new_twit.copy(deep = True)\n",
    "count_df4 = integerwatch_df.copy(deep = True)\n",
    "\n",
    "count_concat = pd.concat([count_df1, count_df2, count_df3, count_df4], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MB1SpPQIbNzY"
   },
   "source": [
    "We extract the text data into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEt_PaX7UBQ6"
   },
   "outputs": [],
   "source": [
    "count_text_data = count_concat['text'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCxDcevHbOs4"
   },
   "source": [
    "We then create a `CountVectorizer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r69JEdwGVhVg"
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KmJjVC6sbO_W"
   },
   "source": [
    "We fit the `count_vectorizer` onto the text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "fO5vxXDKVkOj",
    "outputId": "ae664b1b-14a2-4acb-fd8d-fa54713d8d2b"
   },
   "outputs": [],
   "source": [
    "count_vectorizer.fit(count_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHUQq-rybPUL"
   },
   "source": [
    "We transform the text data into bag-of-words matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TzqrEZQVkGK"
   },
   "outputs": [],
   "source": [
    "bow_matrix = count_vectorizer.transform(count_text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_mXmB7nbQAt"
   },
   "source": [
    "Lastly, we add the bag-of-words matrix as columns in the `count_master` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6Xx4bd0Vj8N"
   },
   "outputs": [],
   "source": [
    "bow_columns = ['bow_{}'.format(i) for i in range(bow_matrix.shape[1])]\n",
    "df_bow = pd.DataFrame(bow_matrix.toarray(), columns=bow_columns)\n",
    "count_master = pd.concat([count_concat, df_bow], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrsafdJfbSoE"
   },
   "source": [
    "We display the head of the `count_master` dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8F1G2CNcVjy1"
   },
   "outputs": [],
   "source": [
    "count_master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwzbULkR8FRB"
   },
   "source": [
    "## **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cut3dtBd-02V"
   },
   "source": [
    "### **EDA Questions:**\n",
    "1. What are the most occurring words under the suicide class?\n",
    "2. What are the most occurring words under the non-suicide class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dpv-i1cfAvsp"
   },
   "source": [
    "A copy of the dataframe containing the combined and tokenized dataset is created for the EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xy9Nc5pqAta8"
   },
   "outputs": [],
   "source": [
    "eda = concat[['class', 'token','text']].copy(deep=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtAaMTd5A0Ad"
   },
   "source": [
    "The eda dataframe is separated into their respective classes: ns for non-suicide (class = 0) and s for suicide (class = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "par-Qb_iA2SW"
   },
   "outputs": [],
   "source": [
    "ns = eda[eda['class'] == 0]\n",
    "s = eda[eda['class'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uxPrwBf-5Ly"
   },
   "source": [
    "#### **What are the most occurring words under the non-suicide class?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dplkKFD3-mpj"
   },
   "outputs": [],
   "source": [
    "text = \" \".join(i for i in ns.text).lower()\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JMyst7XhA8Ng"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow( wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eDgdu48HA8T-"
   },
   "outputs": [],
   "source": [
    "sp = spacy.load('en_core_web_sm')\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "new_stopwords_ns=[\"filler\", \" \", \"S\", \"t\", \"s\", \"m\"]\n",
    "comb_stopwords_ns=list(new_stopwords_ns)+list(all_stopwords)\n",
    "wordcloud = WordCloud(stopwords=comb_stopwords_ns, background_color=\"white\").generate(text)\n",
    "print(new_stopwords_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z9zE1PAiA8Zg"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RejBG2PaA8fo"
   },
   "outputs": [],
   "source": [
    "txt_ns = \" \".join(ns['text'])\n",
    "words_ns = word_tokenize(txt_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i44SZ_VUA8mQ"
   },
   "outputs": [],
   "source": [
    "def cleaned_words(new_tokens):\n",
    "\tnew_tokens = [t.lower() for t in new_tokens]\n",
    "\tnew_tokens =[t for t in new_tokens if t not in stopwords.words('english') and comb_stopwords_ns]\n",
    "\tnew_tokens = [t for t in new_tokens if t.isalpha()]\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tnew_tokens = [lemmatizer.lemmatize(t) for t in new_tokens]\n",
    "\treturn new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j60zgXoPA8tM"
   },
   "outputs": [],
   "source": [
    "lowered_ns = cleaned_words(words_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3DwrywFA9Hc"
   },
   "outputs": [],
   "source": [
    "bow_ns = Counter(lowered_ns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GeUVEp2Z-mzf"
   },
   "outputs": [],
   "source": [
    "data_ns = pd.DataFrame(bow_ns.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "data_ns = data_ns.head(20)\n",
    "sns.barplot(x='frequency',y='word',data=data_ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYnywLixBOnJ"
   },
   "source": [
    "#### **What are the most occurring words under the suicide class?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7FMbO_SBO-O"
   },
   "outputs": [],
   "source": [
    "text_s = \" \".join(i for i in s.text).lower()\n",
    "wordcloud_s = WordCloud(background_color=\"white\").generate(text_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KqRLP_NABUML"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow( wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XahexWshBa15"
   },
   "outputs": [],
   "source": [
    "new_stopwords_s=[\"filler\", \" \", \"S\", \"t\", \"s\", \"m\"]\n",
    "comb_stopwords_s=list(new_stopwords_s)+list(all_stopwords)\n",
    "wordcloud_s = WordCloud(stopwords=comb_stopwords_s, background_color=\"white\").generate(text)\n",
    "print(new_stopwords_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CytZuNIpBURw"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(wordcloud_s, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZZRAqIwBUWz"
   },
   "outputs": [],
   "source": [
    "txt_s = \" \".join(s['text'])\n",
    "words_s = word_tokenize(txt_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HdkCkaOtBUbm"
   },
   "outputs": [],
   "source": [
    "def cleaned_words_s(new_tokens_s):\n",
    "\tnew_tokens_s = [t.lower() for t in new_tokens_s]\n",
    "\tnew_tokens_s =[t for t in new_tokens_s if t not in stopwords.words('english') and comb_stopwords_s]\n",
    "\tnew_tokens_s = [t for t in new_tokens_s if t.isalpha()]\n",
    "\tlemmatizer = WordNetLemmatizer()\n",
    "\tnew_tokens_s = [lemmatizer.lemmatize(t) for t in new_tokens_s]\n",
    "\treturn new_tokens_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWHgBjV1BUgV"
   },
   "outputs": [],
   "source": [
    "lowered_s = cleaned_words_s(words_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3NT8fPzBUlQ"
   },
   "outputs": [],
   "source": [
    "bow_s = Counter(lowered_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lKGoYVsHBUqq"
   },
   "outputs": [],
   "source": [
    "data_s = pd.DataFrame(bow_s.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "data_s = data_s.head(20)\n",
    "sns.barplot(x='frequency',y='word',data=data_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1hBnLUU8lTX"
   },
   "source": [
    "## **Modeling and Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OtdB4OW9F0i"
   },
   "source": [
    "### **Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O00EdUBk9Fy3"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2JveVLtd8lRn"
   },
   "source": [
    "#### **Model Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eoh9QqFq8tWp"
   },
   "source": [
    "#### **Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWZe_k6I9K9I"
   },
   "source": [
    "### **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wpD9w69V8tH9"
   },
   "source": [
    "#### **Feature Importance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pfb9e1p38syL"
   },
   "source": [
    "## **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3SpGHQ9g9jQq"
   },
   "source": [
    "## **Try out our model!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFjsHPDn9VN8"
   },
   "source": [
    "## References"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8zBoR7vo738e",
    "q-XfJf6p7-Pc",
    "8zYEi6d5ooAV",
    "y4PGobu8vBNU",
    "3SqgyWNIRGXe",
    "FgaWLGzkRPwh",
    "_ys69wdN8U44",
    "dBDZg8I8KF6v",
    "J6nFnPeTmtLu",
    "U83pYVOnC2HJ",
    "OwBLpdAwCsUM",
    "QTPJ1W4ICsl6",
    "q1hBnLUU8lTX",
    "9OtdB4OW9F0i",
    "EWZe_k6I9K9I"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
