{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e70ac4",
   "metadata": {},
   "source": [
    "# **Insert Title Here**\n",
    "**DATA103 S11 Group 4**\n",
    "- GOZON, Jean Pauline D.\n",
    "- JAMIAS, Gillian Nicole A.\n",
    "- MARCELO Andrea Jean C. \n",
    "- REYES, Anton Gabriel G.\n",
    "- VICENTE, Francheska Josefa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8a36f",
   "metadata": {},
   "source": [
    "## Requirements and Imports\n",
    "\n",
    "Before starting, the relevant libraries and files in building and training the model should be loaded into the notebook first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02257198",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Several libraries are required to perform a thorough analysis of the dataset. Each of these libraries will be imported and described below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d6f11",
   "metadata": {},
   "source": [
    "**Basic Libraries**\n",
    "\n",
    "Import `numpy`, `pandas`, and `datasets`.\n",
    "\n",
    "* `numpy` contains a large collection of mathematical functions\n",
    "* `pandas` contains functions that are designed for data manipulation and data analysis\n",
    "* `datasets` contains functions that allow easier pre-processing for datasets and smart caching for easier loading of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b421a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab390a",
   "metadata": {},
   "source": [
    "**Machine Learning Libraries**\n",
    "\n",
    "The `train_test_split` is a function that allows the dataset to be split into two randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99272409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86704d1a",
   "metadata": {},
   "source": [
    "Meanwhile, the following imports are used to create the dataset :\n",
    "* `torch` library is an open source ML library for deep neural network creation\n",
    "* `Dataset` and `DataLoader` are two data primitives that makes loading and using dataset easier\n",
    "* `RandomSampler` and `SequentialSampler` are samplers that is used by the `DataLoader`\n",
    "* `ProgressBarBase` and `RichProgressBar` are components that shows the progress bar of training the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4873b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_lightning.callbacks import ProgressBarBase, RichProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d649634",
   "metadata": {},
   "source": [
    "The next imports are from `transformers`, which contains pre-trained models and tokenizers that can be fine-tuned.\n",
    "* `AutoTokenizer` automatically creates the tokenizer based on the architecture passed\n",
    "* `AutoModelForSequenceClassification` automatically instantiates a sequence classification model based on the type of model passed\n",
    "* `TrainerCallback` is an object that determines how the training loop will behave\n",
    "* `TrainingArguments` is a dataclass that allows the customization of the arguments in training\n",
    "* `Trainer` is a class that has a complete training and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7a2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
    "                          TrainerCallback, TrainingArguments, Trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832cde81",
   "metadata": {},
   "source": [
    "On the other hand, these classes computes and visualizes the different scores about how well a model works.\n",
    "* `f1_score` computes the balanced F-score by comparing the actual classes and the predicted classes\n",
    "* `hamming_loss` computes the fraction of labels that were incorrectly labeled by the model\n",
    "* `accuracy_score` computes the accuracy by determining how many classes were correctly predicted\n",
    "* `EvalPrediction` is an object in transformers that holds the prediction of the model and the target output\n",
    "* `evaluate` is a libray that is used to evaluate and compare metrics\n",
    "* `load_metric` is a function in the datasets library that allows different metrics to be loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eee55fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, hamming_loss, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import evaluate\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc9bab1",
   "metadata": {},
   "source": [
    "Next, `optuna` is used to tune the hyperparameters of machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42ff241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a118d6",
   "metadata": {},
   "source": [
    "Last, `pickle` is a module that can serialize and deserialize objects. In this notebook, it is used to save and load models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e848d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb1e919",
   "metadata": {},
   "source": [
    "### Datasets and Files\n",
    "To train the BERT and RoBERTa model, let us load the cleaned dataset with minimual pre-processing using the [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7300021",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['Its not a viable option, and youll be leavin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['It can be hard to appreciate the notion that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>['Hi, so last night i was sitting on the ledge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>['I tried to kill my self once and failed badl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>['Hi NEM3030. What sorts of things do you enjo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242155</th>\n",
       "      <td>0</td>\n",
       "      <td>If you don't like rock then your not going to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242156</th>\n",
       "      <td>0</td>\n",
       "      <td>You how you can tell i have so many friends an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242157</th>\n",
       "      <td>0</td>\n",
       "      <td>pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242158</th>\n",
       "      <td>1</td>\n",
       "      <td>The usual stuff you find hereI'm not posting t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242159</th>\n",
       "      <td>0</td>\n",
       "      <td>I still haven't beaten the first boss in Hollo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242160 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class                                               text\n",
       "0           0  ['Its not a viable option, and youll be leavin...\n",
       "1           1  ['It can be hard to appreciate the notion that...\n",
       "2           1  ['Hi, so last night i was sitting on the ledge...\n",
       "3           1  ['I tried to kill my self once and failed badl...\n",
       "4           1  ['Hi NEM3030. What sorts of things do you enjo...\n",
       "...       ...                                                ...\n",
       "242155      0  If you don't like rock then your not going to ...\n",
       "242156      0  You how you can tell i have so many friends an...\n",
       "242157      0  pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...\n",
       "242158      1  The usual stuff you find hereI'm not posting t...\n",
       "242159      0  I still haven't beaten the first boss in Hollo...\n",
       "\n",
       "[242160 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv ('cleaned_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cca6183",
   "metadata": {},
   "source": [
    "Before we start directly dealing with the data, we will set the **device** on where the model will run. If there is an existence of a CUDA-enabled device, it will automatically pick CUDA as its device. Otherwise, it will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "060c830c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc7c89",
   "metadata": {},
   "source": [
    "## Preparing data for Feature Engineering\n",
    "\n",
    "Before creating the features that the BERT and RoBERTa models will use for training, there are two steps that we must first do: (1) splitting the dataset into the train, val, and test sets, and (2) transforming our [`DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) into a [`Dataset`](https://pypi.org/project/datasets/). This would allow us to utilize the data for the training more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea6faa1",
   "metadata": {},
   "source": [
    "### Splitting the Dataset into Train, Val, and Test Split\n",
    "Let us first define the **X** (input) and **y** (target/output) of our model. This is done to allow the stratifying of the data when it is split into the train, val and test.\n",
    "\n",
    "The **X** (input) can be retrieved by getting the `text` column in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2d8078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ['Its not a viable option, and youll be leavin...\n",
       "1         ['It can be hard to appreciate the notion that...\n",
       "2         ['Hi, so last night i was sitting on the ledge...\n",
       "3         ['I tried to kill my self once and failed badl...\n",
       "4         ['Hi NEM3030. What sorts of things do you enjo...\n",
       "                                ...                        \n",
       "242155    If you don't like rock then your not going to ...\n",
       "242156    You how you can tell i have so many friends an...\n",
       "242157    pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...\n",
       "242158    The usual stuff you find hereI'm not posting t...\n",
       "242159    I still haven't beaten the first boss in Hollo...\n",
       "Name: text, Length: 242160, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df ['text']\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cfa14a",
   "metadata": {},
   "source": [
    "Meanwhile, the **y** value (i.e., the value that we would be \"feeding\" our models) is the `class` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83e11442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         1\n",
       "         ..\n",
       "242155    0\n",
       "242156    0\n",
       "242157    0\n",
       "242158    1\n",
       "242159    0\n",
       "Name: class, Length: 242160, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df ['class']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02546fd",
   "metadata": {},
   "source": [
    "Now that we have declared the input and the target output of our models, we can use the [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to divide the dataset into two splits. Some things to note are: (1) the split is stratified based on the **y values**, (2) the value of the random state was set to 42 for reproducibility, and (3) the dataset is shuffled.\n",
    "\n",
    "First, let us create the train and test set. The test set is made up of 20% of the original dataset, which infers that the second split is 80% of the original. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "314aab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state = 42, \n",
    "                                                    shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2076432c",
   "metadata": {},
   "source": [
    "Second, we will be splitting the remaining 80% of the original dataset into two: the train and val sets. The train set will be 90% of the second split, while the val set will be 10% of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aeb7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                  y_train, \n",
    "                                                  test_size = 0.1,\n",
    "                                                  stratify = y_train,\n",
    "                                                  random_state = 42, \n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e037a8",
   "metadata": {},
   "source": [
    "To check if the shapes of the input and output are the same, we will be looking at the shapes of the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2b8deec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "Input  shape:  (174355,)\n",
      "Output shape:  (174355,) \n",
      "\n",
      "Val\n",
      "Input  shape:  (19373,)\n",
      "Output shape:  (19373,) \n",
      "\n",
      "Test\n",
      "Input  shape:  (48432,)\n",
      "Output shape:  (48432,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "print('Input  shape: ', X_train.shape)\n",
    "print('Output shape: ', y_train.shape, '\\n')\n",
    "\n",
    "print('Val')\n",
    "print('Input  shape: ', X_val.shape)\n",
    "print('Output shape: ', y_val.shape, '\\n')\n",
    "\n",
    "print('Test')\n",
    "print('Input  shape: ', X_test.shape)\n",
    "print('Output shape: ', y_test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869e7e29",
   "metadata": {},
   "source": [
    "As we have already split the data into three (i.e., train, val, test) sets, we can now combine the **X** and **y** values per set through the use of [`concat`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html). This is done for easier tokenizing of the dataset when using BERT and RoBERTa. In addition, using the [`reset_index`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.reset_index.html) function, we would also be resetting the index to make it sequential starting from 0. \n",
    "\n",
    "First, we would concatenate the **X** and **y** values of the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8cd9f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do you explain to your family that you wer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I DONT UNDERSTAND THE US DEBT WHO DO THEY OWE ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FireIt‚Äôs been a bit but I still think of her a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AITA for telling my wife (34F) that reddit agr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Join among us SGGFIF Jesjeuejjejejeeieieijdjdj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174350</th>\n",
       "      <td>Fellow teenagers, I have been influenced by th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174351</th>\n",
       "      <td>I felt like talkingSo I was just outside at 01...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174352</th>\n",
       "      <td>i am trying to but i just cant i have everythi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174353</th>\n",
       "      <td>I just want my suffering to endAll I have hear...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174354</th>\n",
       "      <td>How can you stand the pain?While I was student...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174355 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  class\n",
       "0       How do you explain to your family that you wer...      0\n",
       "1       I DONT UNDERSTAND THE US DEBT WHO DO THEY OWE ...      0\n",
       "2       FireIt‚Äôs been a bit but I still think of her a...      1\n",
       "3       AITA for telling my wife (34F) that reddit agr...      0\n",
       "4       Join among us SGGFIF Jesjeuejjejejeeieieijdjdj...      0\n",
       "...                                                   ...    ...\n",
       "174350  Fellow teenagers, I have been influenced by th...      0\n",
       "174351  I felt like talkingSo I was just outside at 01...      1\n",
       "174352  i am trying to but i just cant i have everythi...      1\n",
       "174353  I just want my suffering to endAll I have hear...      1\n",
       "174354  How can you stand the pain?While I was student...      1\n",
       "\n",
       "[174355 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([X_train, y_train], axis = 1).reset_index(drop = True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fee6f",
   "metadata": {},
   "source": [
    "Next, let us combine for the val (i.e., validation) set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "437f17b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Really down........just need some words of enc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I‚Äôm not gonna buy a carThe day gets closer. I‚Äô...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Help me kill myself. Please. Please. Please.I‚Äô...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The only thing keeping me alive is the fact th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm not.I'm not the sweet, determined girl eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19368</th>\n",
       "      <td>when she says Hi! This post seems to be relate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19369</th>\n",
       "      <td>I gotta go to school tmmr for orientation at 9...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19370</th>\n",
       "      <td>Hey lads! Can I get some help from y'all? So.....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19371</th>\n",
       "      <td>My birthday is this coming month and it will b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19372</th>\n",
       "      <td>Posting songs I like day 20 https://youtu.be/i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19373 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  class\n",
       "0      Really down........just need some words of enc...      1\n",
       "1      I‚Äôm not gonna buy a carThe day gets closer. I‚Äô...      1\n",
       "2      Help me kill myself. Please. Please. Please.I‚Äô...      1\n",
       "3      The only thing keeping me alive is the fact th...      1\n",
       "4      I'm not.I'm not the sweet, determined girl eve...      1\n",
       "...                                                  ...    ...\n",
       "19368  when she says Hi! This post seems to be relate...      0\n",
       "19369  I gotta go to school tmmr for orientation at 9...      0\n",
       "19370  Hey lads! Can I get some help from y'all? So.....      0\n",
       "19371  My birthday is this coming month and it will b...      1\n",
       "19372  Posting songs I like day 20 https://youtu.be/i...      0\n",
       "\n",
       "[19373 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.concat([X_val, y_val], axis = 1).reset_index(drop = True)\n",
    "val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bada419",
   "metadata": {},
   "source": [
    "Last, we would also be doing these same steps to the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cbabe1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I just felt myself snapI have to pretend to be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are you envious of something about the opposit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We get it. Men have problems, too. We never sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy Birthday to everyone having Birthday on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i cant deal with life any longer but ive tried...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48427</th>\n",
       "      <td>I just need to go for everyone's sakeI can't e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48428</th>\n",
       "      <td>Hope is now goneI'm 17m and I'm considering ta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48429</th>\n",
       "      <td>18f needs someone to talk toI understand if th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48430</th>\n",
       "      <td>Help mePlease someone help me, just pm me.\\nI'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48431</th>\n",
       "      <td>bf application üòé yo. i‚Äôm 17 i‚Äôm a gorl. and th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48432 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  class\n",
       "0      I just felt myself snapI have to pretend to be...      1\n",
       "1      Are you envious of something about the opposit...      0\n",
       "2      We get it. Men have problems, too. We never sa...      0\n",
       "3      Happy Birthday to everyone having Birthday on ...      0\n",
       "4      i cant deal with life any longer but ive tried...      1\n",
       "...                                                  ...    ...\n",
       "48427  I just need to go for everyone's sakeI can't e...      1\n",
       "48428  Hope is now goneI'm 17m and I'm considering ta...      1\n",
       "48429  18f needs someone to talk toI understand if th...      1\n",
       "48430  Help mePlease someone help me, just pm me.\\nI'...      1\n",
       "48431  bf application üòé yo. i‚Äôm 17 i‚Äôm a gorl. and th...      0\n",
       "\n",
       "[48432 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.concat([X_test, y_test], axis = 1).reset_index(drop = True)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61a052",
   "metadata": {},
   "source": [
    "### Creation of Dataset\n",
    "Since we have already created three different sets, we can now transform our DataFrames into one single Dataset. To do this, we first have to transform each set into a single dataset before combining them into one dataset.\n",
    "\n",
    "First, we would be converting out train DataFrame into a dataset. In this, it can be seen that there are **174,355** rows in our train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c555f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'class'],\n",
       "    num_rows: 174355\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasets.Dataset.from_pandas(train_df)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b7a41",
   "metadata": {},
   "source": [
    "This is followed by transforming the val DataFrame also. This would result in a dataset with **19,373** rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3092821e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'class'],\n",
       "    num_rows: 19373\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = datasets.Dataset.from_pandas(val_df)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9294b67d",
   "metadata": {},
   "source": [
    "Last is the test DataFrame, which would become a dataset with **48,432** rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ac0956b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'class'],\n",
       "    num_rows: 48432\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = datasets.Dataset.from_pandas(test_df)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607e3fb1",
   "metadata": {},
   "source": [
    "As we now have a dataset form for all of our sets, we can now merge them together into one dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "673bbef5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'class'],\n",
       "        num_rows: 174355\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'class'],\n",
       "        num_rows: 19373\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'class'],\n",
       "        num_rows: 48432\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.DatasetDict({\n",
    "    \"train\" : train_dataset, \n",
    "    \"val\" : val_dataset, \n",
    "    \"test\" : test_dataset\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c95903",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Because we are done preparing our data, we can now start with transforming it into a form that the machine learning algorithms can understand through feature engineering. For this notebook, we will be utilizing tokenization, specifically through the use of BERT and RoBERTa tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80bccf6",
   "metadata": {},
   "source": [
    "### Defining of Functions and Values\n",
    "Before starting with the tokenizing itself, we will first have to define the needed functions and values. \n",
    "\n",
    "One of these values is the **MAX_LENGTH**, which determines the maximum length that will be allowed by the model. This means that it will be used by the tokenizer in two ways: (1) inputs that are longer than this length will be truncated to this value, and (2) inputs that are shorter than this length will be padded so that it will reach this length. For this notebook, **512** is set as the maximum length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d9d4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e4cd1",
   "metadata": {},
   "source": [
    "In addition, the preprocessing function for an instance is created. In this function, a text is tokenized by the tokenizer (i.e., padded and truncated to the maximum length) and its corresponding label is transformed into a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fca978aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer):\n",
    "    encoding = tokenizer(examples[\"text\"], padding = \"max_length\", truncation = True, max_length = MAX_LENGTH)\n",
    "    encoding[\"labels\"] = torch.tensor(examples ['class'])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434646b7",
   "metadata": {},
   "source": [
    "Last, the function that would call the preprocessing function on the dataset is defined. In this function, the dataset is also set into a **torch** format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82755943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoded_dataset (tokenizer):\n",
    "    encoded_dataset = dataset.map(preprocess_function, \n",
    "                                  batched=True, \n",
    "                                  remove_columns=dataset['train'].column_names, \n",
    "                                  fn_kwargs = {\"tokenizer\": tokenizer})\n",
    "    \n",
    "    encoded_dataset.set_format(\"torch\")\n",
    "    \n",
    "    return encoded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900fffaf",
   "metadata": {},
   "source": [
    "### Tokenizing with BERT\n",
    "As our functions and values are ready, the tokenizer can be instantiated. Since we would be utilizing a BERT model, specifically the **bert-base-cased** model, we would be creating a tokenizer that can prepare the text data into the input accepted by the model. \n",
    "\n",
    "This can be done through the [`AutoTokenizer`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer) class and the `from_pretrained` function, since the model and the tokenizer that we want to use has already been pretrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "48eeead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', use_fast = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd05ac1f",
   "metadata": {},
   "source": [
    "With this tokenizer, we will be encoding the dataset into the correct form that is needed by the BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cba846e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115ee43c2d934175835a016500f695ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea336832acb945e19543fb5df27d0259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c52476e65e45de8475c7911a3ebffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_encoded_dataset = create_encoded_dataset (bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787036ae",
   "metadata": {},
   "source": [
    "### Tokenizing with RoBERTa\n",
    "Next, as we also want to use a pretrained RoBERTa model (i.e., **roberta-base**), we also have to do the same steps.\n",
    "\n",
    "To start with, we need to create an instance of the specific RoBERTa model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c7e5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb13a8",
   "metadata": {},
   "source": [
    "Since we already have an instance of the tokenizer, we can now use this tokenizer and the pre-processing function we defined previously to transform the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab5b42a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbeef61a82a4b74896592a8e1019497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dabfb3e4140447a6a1b6d71ea1cb51c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432c6a8bf2fc43ef81d17dc6cc133a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_encoded_dataset = create_encoded_dataset (roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779da9d",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation\n",
    "\n",
    "As we have already created the features that we would be using for our models, we can now proceed with the modeling proper. For this project, we would be fine-tuning two pre-trained models: **BERT** and **RoBERTa**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f4719",
   "metadata": {},
   "source": [
    "### Defining of Functions and Values\n",
    "\n",
    "Before we start with the training proper, we would need to define the functions that will be used for training and evaluating. \n",
    "\n",
    "First, we would be creating the function that would be used to compute the scores of the model. In this, we would be using four metrics to evaluate our models: (1) **F1 Macro Score**, (2) **Accuracy**, (3) **Precision**, and (4) **Recall**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6a9b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision_metric = load_metric(\"precision\")\n",
    "    recall_metric = load_metric(\"recall\")\n",
    "    accuracy_metric = load_metric(\"accuracy\")\n",
    "    f1_metric = load_metric(\"f1\")\n",
    "    \n",
    "    f1_macro_score = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    accuracy_score = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision_score = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    recall_score = recall_metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    results = {\n",
    "        'Accuracy' : accuracy_score ['accuracy'],\n",
    "        'F1 Macro Score' : f1_macro_score ['f1'], \n",
    "        'Precision' : precision_score[\"precision\"],\n",
    "        'Recall' : recall_score[\"recall\"]\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a535459",
   "metadata": {},
   "source": [
    "Second, we would be specifying the hyperparameter space that would determine the possible hyperparameter vaues to be tuned. In this, only three hyperparameters would be considered for tuning: (1) the **learning rate**, (2) the **train batch size**, and (3) the **number of training epochs**.\n",
    "\n",
    "Note that the combination of values would be randomized from the sets of values, and there would only be three combinations that would be used for the tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "50a5cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.1, 0.01, 0.001]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16]),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [2, 3, 4])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb8827",
   "metadata": {},
   "source": [
    "### BERT Model\n",
    "Now, we are ready to move on to training the BERT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841056d",
   "metadata": {},
   "source": [
    "#### Model Training \n",
    "\n",
    "To start with, let us define the pre-trained model that we would be using. For the BERT, [**bert-base-cased**](https://huggingface.co/bert-base-cased)‚Äîa model that was pre-trained on a case-sensitive English corpus for masked language modeling (MLM)‚Äîwould be utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49f8a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3ff527",
   "metadata": {},
   "source": [
    "Let us create an instance of a BERT model using this pretrained model. \n",
    "\n",
    "As we would be fine-tuning this model to classify text (i.e., if it is a suicidal or non-suicidal text), an instance of [`AutoModelForSequenceClassification`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification) would be created specifically. It is also important to note that the input that it would accept is based on the **MAX_LENGTH** variable that we have previously declare, which has the value of **512**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7dcd65db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels = 2, \n",
    "    max_length = MAX_LENGTH\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8dff9d",
   "metadata": {},
   "source": [
    "Next, we would be defining the [`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) that the [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) would be using. The parameters for the [`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) that are used for the training loop are as follows:\n",
    "* `output_dir` indicates that the model predictions and checkpoints will be saved in the **bert_trainer** folder\n",
    "* `save_steps` means that the checkpoint will be saved every **20,000** steps\n",
    "* `save_strategy` specifies that the saving of checkpoint will be based on the number of steps that the model has done \n",
    "* `fp16` stipulates that the **16-bit floating point precision** will be used (since its value is True) to save memory\n",
    "* `evaluation_strategy` designates that the **evaluation** should be done **every after epochs**\n",
    "* `resume_from_checkpoint` indicates that the training could be **restarted from a previous checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5940d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = \"bert_trainer\", \n",
    "                                  save_steps = 20000,\n",
    "                                  save_strategy = 'steps',\n",
    "                                  fp16 = True,\n",
    "                                  evaluation_strategy = \"epoch\", \n",
    "                                  resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0853dbab",
   "metadata": {},
   "source": [
    "As we have now declared the pre-trained model and the training arguments that we would be using, we can now instantiate a [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) that can do training and evaluation using the following parameters:\n",
    "* `model` is the BERT model that we would be using for sequence classification\n",
    "* `args` holds the training arguments that we have previously defined\n",
    "* `train_dataset` is the tokenized dataset that we would be using for training\n",
    "* `eval_dataset` is the tokenized dataset that we would be using for evaluating (i.e., the val set)\n",
    "* `tokenizer` is the tokenizer that we used to prepare our data for the BERT model\n",
    "* `compute_metrics` is the function that the evaluation loop would use to score the model\n",
    "* `callbacks` holds the **ProgressBar**, which would allow us to see the progress of our model in training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6ba97d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = bert_model,\n",
    "    args = training_args,\n",
    "    train_dataset = bert_encoded_dataset ['train'],\n",
    "    eval_dataset = bert_encoded_dataset ['val'],\n",
    "    tokenizer = bert_tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [TrainerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c02cb",
   "metadata": {},
   "source": [
    "Using the instance of [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) that we have created, we can now fine-tune the pre-trained BERT model through the use of the [`train`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.train) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43cb5bc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65385\n",
      "  Number of trainable parameters = 108311810\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancheska_vicente\u001b[0m (\u001b[33mtonely\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230406_205221-7esay8x9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/7esay8x9' target=\"_blank\">changeling-blood-wine-109</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/7esay8x9' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/7esay8x9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65385' max='65385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65385/65385 5:33:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.367400</td>\n",
       "      <td>0.413518</td>\n",
       "      <td>0.882930</td>\n",
       "      <td>0.882769</td>\n",
       "      <td>0.855347</td>\n",
       "      <td>0.921319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.561000</td>\n",
       "      <td>1.071023</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>0.162171</td>\n",
       "      <td>0.952924</td>\n",
       "      <td>0.952922</td>\n",
       "      <td>0.957012</td>\n",
       "      <td>0.948304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert_trainer\\checkpoint-20000\n",
      "Configuration saved in bert_trainer\\checkpoint-20000\\config.json\n",
      "Model weights saved in bert_trainer\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\checkpoint-20000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_8112\\1346387678.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  precision_metric = load_metric(\"precision\")\n",
      "Saving model checkpoint to bert_trainer\\checkpoint-40000\n",
      "Configuration saved in bert_trainer\\checkpoint-40000\\config.json\n",
      "Model weights saved in bert_trainer\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\checkpoint-40000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert_trainer\\checkpoint-60000\n",
      "Configuration saved in bert_trainer\\checkpoint-60000\\config.json\n",
      "Model weights saved in bert_trainer\\checkpoint-60000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\checkpoint-60000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\checkpoint-60000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=65385, training_loss=0.3582221678567332, metrics={'train_runtime': 20003.5531, 'train_samples_per_second': 26.149, 'train_steps_per_second': 3.269, 'total_flos': 1.376241841718784e+17, 'train_loss': 0.3582221678567332, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee220550",
   "metadata": {},
   "source": [
    "From the result above, we can see that the model received the highest evaluation score on the validation set on the third epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e5de1c",
   "metadata": {},
   "source": [
    "#### Saving BERT base model\n",
    "To use this model outside the notebook, we would be saving the model. First, let us define the folder where we would be saving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c84dd6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_for_models ='./saved_models/BERTv4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71290ce5",
   "metadata": {},
   "source": [
    "Now, let us save the [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) (i.e., with the weights, the configurations, and the model) and the [`BertTokenizer`](https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertTokenizer) in the specified folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "331f536c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_models/BERTv4\n",
      "Configuration saved in ./saved_models/BERTv4\\config.json\n",
      "Model weights saved in ./saved_models/BERTv4\\pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/BERTv4\\tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/BERTv4\\special_tokens_map.json\n",
      "tokenizer config file saved in ./saved_models/BERTv4\\tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/BERTv4\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_models/BERTv4\\\\tokenizer_config.json',\n",
       " './saved_models/BERTv4\\\\special_tokens_map.json',\n",
       " './saved_models/BERTv4\\\\vocab.txt',\n",
       " './saved_models/BERTv4\\\\added_tokens.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(path_for_models)\n",
    "bert_tokenizer.save_pretrained(path_for_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93672b7d",
   "metadata": {},
   "source": [
    "Using the [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) we have trained, we can now [`evaluate`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.evaluate) the model using the test set to determine its test score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63aa0e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 48432\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6054' max='6054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6054/6054 08:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_28132\\730230934.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  precision_metric = load_metric(\"precision\")\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancheska_vicente\u001b[0m (\u001b[33mtonely\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230410_130002-p6e969l2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/p6e969l2' target=\"_blank\">vivid-bird-122</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/p6e969l2' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/p6e969l2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.16548407077789307,\n",
       " 'eval_Accuracy': 0.9506111661711265,\n",
       " 'eval_F1 Macro Score': 0.9506109295912639,\n",
       " 'eval_Precision': 0.9511326458773347,\n",
       " 'eval_Recall': 0.949873857479631,\n",
       " 'eval_runtime': 545.5507,\n",
       " 'eval_samples_per_second': 88.776,\n",
       " 'eval_steps_per_second': 11.097}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=bert_encoded_dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aeef73",
   "metadata": {},
   "source": [
    "From the result above, it can be seen that the model was able to be correctly trained. It achieved the following scores: 95.06% for Accuracy and F1 Macro Score, 95.11% for Precision, and 94.99% for Recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb64f4",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "Now, let us try to tune the hyperparameters (i.e., the learning rate, the number of training epochs and the training batch size) of the model, which means that we would try to find the value that would give us the highest score. In this, we would be trying three combinations of these hyperparameters, and we would compare the scores received by the three combinations to the score of the base model. \n",
    "\n",
    "To do this, we will first create a function that would return a base model of a BERT [`AutoModelForSequenceClassification`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification) for initializaiton. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbb450f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n",
    "                                                              num_labels = 2, \n",
    "                                                              max_length = MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0ef38",
   "metadata": {},
   "source": [
    "Like in training the base model, we would be creating the [`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) that we would be using for training. We would be using the same parameters for the [`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) as before, except for the **fp16**. \n",
    "\n",
    "In the tuning, **bf16** (bfloat16) will be used. This was done because using **fp16** resulted in 0.0 scores due to the loss of floating points in fp16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7530d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_tuning = TrainingArguments(output_dir = \"bert_trainer\", \n",
    "                                         save_steps = 20000, \n",
    "                                         bf16 = True,\n",
    "                                         save_strategy = 'steps',\n",
    "                                         evaluation_strategy = \"epoch\", \n",
    "                                         resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21250177",
   "metadata": {},
   "source": [
    "Next, we can create an instance of [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) class. Since we would be using the [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) for tuning, we passed an **initialization of the model** instead of a model. This initial model is used as the base (i.e., the model is reinitialized every run of new hyperparameter values). This means that all of the models use the values of the base model and only the values of the hyperparameter passed are changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3e177b2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer_tuning = Trainer(\n",
    "    model_init = model_init,\n",
    "    args = training_args_tuning,\n",
    "    train_dataset = bert_encoded_dataset ['train'],\n",
    "    eval_dataset = bert_encoded_dataset ['val'],\n",
    "    tokenizer = bert_tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [TrainerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eafa14",
   "metadata": {},
   "source": [
    "Using the [`hyperparameter_search`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.hyperparameter_search) function, we can now start finding the best values of the hyperparameters to use. Note that this function will return the information about the best run (i.e., the model that received the best score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3358c16f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 06:36:09,875]\u001b[0m A new study created in memory with name: no-name-8501078e-7df7-41fd-b022-6dc74c71cc6e\u001b[0m\n",
      "Trial: {'learning_rate': 0.001, 'per_device_train_batch_size': 8, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65385\n",
      "  Number of trainable parameters = 108311810\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancheska_vicente\u001b[0m (\u001b[33mtonely\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230407_063613-7ncnmqqu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/7ncnmqqu' target=\"_blank\">nemesis-unimatrix-110</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/7ncnmqqu' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/7ncnmqqu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65385' max='65385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65385/65385 5:33:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.751700</td>\n",
       "      <td>0.710669</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.720200</td>\n",
       "      <td>0.695365</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.697200</td>\n",
       "      <td>0.691395</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert_trainer\\run-0\\checkpoint-20000\n",
      "Configuration saved in bert_trainer\\run-0\\checkpoint-20000\\config.json\n",
      "Model weights saved in bert_trainer\\run-0\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-0\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-0\\checkpoint-20000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20460\\730230934.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  precision_metric = load_metric(\"precision\")\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert_trainer\\run-0\\checkpoint-40000\n",
      "Configuration saved in bert_trainer\\run-0\\checkpoint-40000\\config.json\n",
      "Model weights saved in bert_trainer\\run-0\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-0\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-0\\checkpoint-40000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert_trainer\\run-0\\checkpoint-60000\n",
      "Configuration saved in bert_trainer\\run-0\\checkpoint-60000\\config.json\n",
      "Model weights saved in bert_trainer\\run-0\\checkpoint-60000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-0\\checkpoint-60000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-0\\checkpoint-60000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-04-07 12:10:01,271]\u001b[0m Trial 0 finished with value: 0.8344142826144729 and parameters: {'learning_rate': 0.001, 'per_device_train_batch_size': 8, 'num_train_epochs': 3}. Best is trial 0 with value: 0.8344142826144729.\u001b[0m\n",
      "Trial: {'learning_rate': 0.1, 'per_device_train_batch_size': 8, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65385\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f24de99e4d84c7ba599028b6adf8acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.041 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.033370‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Accuracy</td><td>‚ñà‚ñÅ‚ñà</td></tr><tr><td>eval/F1 Macro Score</td><td>‚ñà‚ñÅ‚ñà</td></tr><tr><td>eval/Precision</td><td>‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/Recall</td><td>‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÇ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÖ‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñÑ‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñÑ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Accuracy</td><td>0.50075</td></tr><tr><td>eval/F1 Macro Score</td><td>0.33367</td></tr><tr><td>eval/Precision</td><td>0.0</td></tr><tr><td>eval/Recall</td><td>0.0</td></tr><tr><td>eval/loss</td><td>0.69139</td></tr><tr><td>eval/runtime</td><td>221.2597</td></tr><tr><td>eval/samples_per_second</td><td>87.558</td></tr><tr><td>eval/steps_per_second</td><td>10.946</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>65385</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.6972</td></tr><tr><td>train/total_flos</td><td>1.376241841718784e+17</td></tr><tr><td>train/train_loss</td><td>0.73432</td></tr><tr><td>train/train_runtime</td><td>20030.0377</td></tr><tr><td>train/train_samples_per_second</td><td>26.114</td></tr><tr><td>train/train_steps_per_second</td><td>3.264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nemesis-unimatrix-110</strong> at: <a href='https://wandb.ai/tonely/huggingface/runs/7ncnmqqu' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/7ncnmqqu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230407_063613-7ncnmqqu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50ba7479b3747e6999806a1f23a9a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666414434, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230407_121008-bv89f701</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/bv89f701' target=\"_blank\">frosty-terrain-111</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/bv89f701' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/bv89f701</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65385' max='65385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65385/65385 5:34:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.033400</td>\n",
       "      <td>11.454621</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.321500</td>\n",
       "      <td>4.412920</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.805800</td>\n",
       "      <td>0.695219</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert_trainer\\run-1\\checkpoint-20000\n",
      "Configuration saved in bert_trainer\\run-1\\checkpoint-20000\\config.json\n",
      "Model weights saved in bert_trainer\\run-1\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-1\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-1\\checkpoint-20000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert_trainer\\run-1\\checkpoint-40000\n",
      "Configuration saved in bert_trainer\\run-1\\checkpoint-40000\\config.json\n",
      "Model weights saved in bert_trainer\\run-1\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-1\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-1\\checkpoint-40000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert_trainer\\run-1\\checkpoint-60000\n",
      "Configuration saved in bert_trainer\\run-1\\checkpoint-60000\\config.json\n",
      "Model weights saved in bert_trainer\\run-1\\checkpoint-60000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-1\\checkpoint-60000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-1\\checkpoint-60000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-04-07 17:44:21,185]\u001b[0m Trial 1 finished with value: 0.8344142826144729 and parameters: {'learning_rate': 0.1, 'per_device_train_batch_size': 8, 'num_train_epochs': 3}. Best is trial 0 with value: 0.8344142826144729.\u001b[0m\n",
      "Trial: {'learning_rate': 0.1, 'per_device_train_batch_size': 8, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65385\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe03299fcbdc4f2d939bc053b5cd343e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Accuracy</td><td>‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>eval/F1 Macro Score</td><td>‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>eval/Precision</td><td>‚ñà‚ñà‚ñÅ</td></tr><tr><td>eval/Recall</td><td>‚ñà‚ñà‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÖ‚ñÅ‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÑ‚ñà‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÑ‚ñà‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Accuracy</td><td>0.50075</td></tr><tr><td>eval/F1 Macro Score</td><td>0.33367</td></tr><tr><td>eval/Precision</td><td>0.0</td></tr><tr><td>eval/Recall</td><td>0.0</td></tr><tr><td>eval/loss</td><td>0.69522</td></tr><tr><td>eval/runtime</td><td>219.0251</td></tr><tr><td>eval/samples_per_second</td><td>88.451</td></tr><tr><td>eval/steps_per_second</td><td>11.058</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>65385</td></tr><tr><td>train/learning_rate</td><td>0.00059</td></tr><tr><td>train/loss</td><td>0.8058</td></tr><tr><td>train/total_flos</td><td>1.376241841718784e+17</td></tr><tr><td>train/train_loss</td><td>5.76942</td></tr><tr><td>train/train_runtime</td><td>20058.3406</td></tr><tr><td>train/train_samples_per_second</td><td>26.077</td></tr><tr><td>train/train_steps_per_second</td><td>3.26</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-terrain-111</strong> at: <a href='https://wandb.ai/tonely/huggingface/runs/bv89f701' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/bv89f701</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230407_121008-bv89f701\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c8924bce4342f4be68bac82406b923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230407_174428-34malvo9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/34malvo9' target=\"_blank\">elated-resonance-112</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/34malvo9' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/34malvo9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65385' max='65385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65385/65385 5:33:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.033400</td>\n",
       "      <td>11.454621</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.321500</td>\n",
       "      <td>4.412920</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.805800</td>\n",
       "      <td>0.695219</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert_trainer\\run-2\\checkpoint-20000\n",
      "Configuration saved in bert_trainer\\run-2\\checkpoint-20000\\config.json\n",
      "Model weights saved in bert_trainer\\run-2\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-2\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-2\\checkpoint-20000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert_trainer\\run-2\\checkpoint-40000\n",
      "Configuration saved in bert_trainer\\run-2\\checkpoint-40000\\config.json\n",
      "Model weights saved in bert_trainer\\run-2\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-2\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-2\\checkpoint-40000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert_trainer\\run-2\\checkpoint-60000\n",
      "Configuration saved in bert_trainer\\run-2\\checkpoint-60000\\config.json\n",
      "Model weights saved in bert_trainer\\run-2\\checkpoint-60000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-2\\checkpoint-60000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-2\\checkpoint-60000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-04-07 23:17:54,665]\u001b[0m Trial 2 finished with value: 0.8344142826144729 and parameters: {'learning_rate': 0.1, 'per_device_train_batch_size': 8, 'num_train_epochs': 3}. Best is trial 0 with value: 0.8344142826144729.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "best_trial = trainer_tuning.hyperparameter_search(\n",
    "    direction = \"maximize\",\n",
    "    backend = \"optuna\",\n",
    "    hp_space = optuna_hp_space,\n",
    "    n_trials = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbd70eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='0', objective=0.8344142826144729, hyperparameters={'learning_rate': 0.001, 'per_device_train_batch_size': 8, 'num_train_epochs': 3})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319a2c94",
   "metadata": {},
   "source": [
    "In this, it can be seen that there were only two BERT models that were created in tuning, with the following hyperparameters:\n",
    "* **Learning Rate** = 0.001, **Train Batch Size** = 8, **Number of Train Epochs** = 3\n",
    "* **Learning Rate** = 0.1, **Train Batch Size** = 8, **Number of Train Epochs** = 3\n",
    "\n",
    "These values were randomly generated based on the hyperparameter space that we have declared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5e1e3",
   "metadata": {},
   "source": [
    "##### Saving BERT tuned model\n",
    "\n",
    "Like in the base model, we will also save the files of the best trial of the tuned model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7ab72ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_models/BERTv2_tuned\n",
      "Configuration saved in ./saved_models/BERTv2_tuned\\config.json\n",
      "Model weights saved in ./saved_models/BERTv2_tuned\\pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/BERTv2_tuned\\tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/BERTv2_tuned\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "path_for_models ='./saved_models/BERTv2_tuned'\n",
    "trainer_tuning.save_model(path_for_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc78564",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "To test how the best trial of the BERT tuning fared in the test dataset, we will be using the [`evaluate`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.evaluate) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1379f892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 48432\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6054' max='6054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6054/6054 17:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_6612\\730230934.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  precision_metric = load_metric(\"precision\")\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancheska_vicente\u001b[0m (\u001b[33mtonely\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230411_131654-iywm6odz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/iywm6odz' target=\"_blank\">sunny-cherry-123</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/iywm6odz' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/iywm6odz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6942650675773621,\n",
       " 'eval_Accuracy': 0.5007639577139081,\n",
       " 'eval_F1 Macro Score': 0.3336726972552796,\n",
       " 'eval_Precision': 0.0,\n",
       " 'eval_Recall': 0.0,\n",
       " 'eval_runtime': 1057.7462,\n",
       " 'eval_samples_per_second': 45.788,\n",
       " 'eval_steps_per_second': 5.723}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_tuning.evaluate(eval_dataset=bert_encoded_dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328e73cc",
   "metadata": {},
   "source": [
    "In this result, it can be seen that the BERT model (with the learning rate of 0.001) was only accurate on 50% of the test samples. Bsed on the precision, this means that if the model predicts that the text is **Suicidal**, it is **correct 0% of the time**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77630873",
   "metadata": {},
   "source": [
    "Comparing the scores of these two models from tuning to the base model in the validation, the scores received by the base model was still better. Note that the only difference between these three models is the **learning rate** (i.e., the BERT base model has a learning rate of **0.0001**). Thus, for the BERT model, we will consider the base model as our best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe26554",
   "metadata": {},
   "source": [
    "### RoBERTa Model\n",
    "Now, we can move on to training the RoBERTa model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a119a",
   "metadata": {},
   "source": [
    "#### Model Training \n",
    "Like in the BERT model, we would need to define the pre-trained model that we would be fine-tuning. For this, we would be using [**roberta-base**](https://huggingface.co/roberta-base). This model, which is case-sensitive, was also pre-trained for the purpose of masked language modeling (MLM) on an English corpus, however, it uses the RoBERTa architecture, instead of the BERT architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55dfa1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_roberta = 'roberta-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92839b72",
   "metadata": {},
   "source": [
    "Using this pre-trained model, we can instantiate a [`AutoModelForSequenceClassification`](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html#automodelforsequenceclassification) object, which will create a RoBERTa model. In addition, we would also be defining the **MAX_LENGTH** of the model to be the same as the previously defined **MAX_LENGTH** (i.e., 512)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd78e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint_roberta,\n",
    "    num_labels = 2, \n",
    "    max_length = MAX_LENGTH\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54634df6",
   "metadata": {},
   "source": [
    "We would also need to create an instance of [`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments). This would have the same values as the previous [`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) of the BERT model, except for the `output_dir`, as we wnat to save the checkpoints in another folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d7d267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = \"roberta_trainer\", \n",
    "                                  save_steps = 20000,\n",
    "                                  save_strategy = 'steps',\n",
    "                                  fp16 = True,\n",
    "                                  evaluation_strategy = \"epoch\", \n",
    "                                  resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819754d8",
   "metadata": {},
   "source": [
    "Using this RoBERTa model and the previously created [`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) object, we can now create a  [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer). Its parameters are also the same with the [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) for BERT, but the `model`, `train_dataset`, and `eval_dataset` are changed to the RoBERTa counterparts.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71ce2c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = roberta_model,\n",
    "    args = training_args,\n",
    "    train_dataset = roberta_encoded_dataset ['train'],\n",
    "    eval_dataset = roberta_encoded_dataset ['val'],\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [TrainerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a261cf79",
   "metadata": {},
   "source": [
    "Now, we can train the RoBERTa model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0364213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65385\n",
      "  Number of trainable parameters = 124647170\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancheska_vicente\u001b[0m (\u001b[33mtonely\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230408_202820-9kktjdac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/9kktjdac' target=\"_blank\">rare-leaf-115</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/9kktjdac' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/9kktjdac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65385' max='65385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65385/65385 5:43:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.622600</td>\n",
       "      <td>1.136392</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.693500</td>\n",
       "      <td>0.934038</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.176044</td>\n",
       "      <td>0.954731</td>\n",
       "      <td>0.954715</td>\n",
       "      <td>0.970673</td>\n",
       "      <td>0.937655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to roberta_trainer\\checkpoint-20000\n",
      "Configuration saved in roberta_trainer\\checkpoint-20000\\config.json\n",
      "Model weights saved in roberta_trainer\\checkpoint-20000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_3484\\730230934.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  precision_metric = load_metric(\"precision\")\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to roberta_trainer\\checkpoint-40000\n",
      "Configuration saved in roberta_trainer\\checkpoint-40000\\config.json\n",
      "Model weights saved in roberta_trainer\\checkpoint-40000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to roberta_trainer\\checkpoint-60000\n",
      "Configuration saved in roberta_trainer\\checkpoint-60000\\config.json\n",
      "Model weights saved in roberta_trainer\\checkpoint-60000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=65385, training_loss=0.5497573370633435, metrics={'train_runtime': 20653.9299, 'train_samples_per_second': 25.325, 'train_steps_per_second': 3.166, 'total_flos': 1.376241841718784e+17, 'train_loss': 0.5497573370633435, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46f322b",
   "metadata": {},
   "source": [
    "From this, it can be seen that, in the third epoch, the RoBERTa base model was able to achieve an **Accuracy and F1 Macro Score** of **95.47%**, a **Precision** of **97.07%**, and a **Recall** of\t**93.77%**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3224c",
   "metadata": {},
   "source": [
    "#### Saving RoBERTa base model\n",
    "Since we are done training the model, we would be saving the RoBERTa model, and its configuration and tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ea34aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_models/RoBERTav2\n",
      "Configuration saved in ./saved_models/RoBERTav2\\config.json\n",
      "Model weights saved in ./saved_models/RoBERTav2\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "path_for_models ='./saved_models/RoBERTav2'\n",
    "trainer.save_model(path_for_models)\n",
    "roberta_tokenizer.save_pretrained(path_for_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b2394",
   "metadata": {},
   "source": [
    "We can now [`evaluate`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.evaluate) this RoBERTa model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3c5b0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 48432\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6054' max='6054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6054/6054 08:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.16810336709022522,\n",
       " 'eval_Accuracy': 0.95701189296333,\n",
       " 'eval_F1 Macro Score': 0.9569997463552207,\n",
       " 'eval_Precision': 0.9713724988267417,\n",
       " 'eval_Recall': 0.9416435750031019,\n",
       " 'eval_runtime': 524.6124,\n",
       " 'eval_samples_per_second': 92.32,\n",
       " 'eval_steps_per_second': 11.54,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=roberta_encoded_dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca56b2e1",
   "metadata": {},
   "source": [
    "Comparing the scores received by the RoBERTa base model and the best BERT model, it is apparent that the **RoBERTa model received higher scores in every metric except for Recall**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d71b6",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "To further see if we can improve the current RoBERTa model, we can tune the model's hyperparameters. \n",
    "\n",
    "Like in the BERT model, we would first need to create a function that would return the initial state of the model that would be tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5adedfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init_roberta ():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint_roberta,\n",
    "                                                              num_labels = 2, \n",
    "                                                              max_length = MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8fe75f",
   "metadata": {},
   "source": [
    "Next, we would have to create the [`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) that we would be using for the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "45512936",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_tuning = TrainingArguments(output_dir = \"roberta_trainer\", \n",
    "                                         save_steps = 20000, \n",
    "                                         bf16 = True,\n",
    "                                         save_strategy = 'steps',\n",
    "                                         evaluation_strategy = \"epoch\", \n",
    "                                         resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f64286",
   "metadata": {},
   "source": [
    "With this, we can now proceed with creating an instance of the [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb52b1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer_tuning = Trainer(\n",
    "    model_init = model_init_roberta,\n",
    "    args = training_args_tuning,\n",
    "    train_dataset = roberta_encoded_dataset ['train'],\n",
    "    eval_dataset = roberta_encoded_dataset ['val'],\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [TrainerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f4b2b7",
   "metadata": {},
   "source": [
    "We can now proceed with utilizing the [`hyperparameter_search`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer.hyperparameter_search) function to: (1) randomize values for the three hyperparameters that we want to tune based on the search space, (2) train three models using the values, and (3) pick the best model from the three trained models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "62f231c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-09 22:02:53,389]\u001b[0m A new study created in memory with name: no-name-8d7ea55e-5a1a-4654-9a1c-e2933a9a9274\u001b[0m\n",
      "Trial: {'learning_rate': 0.01, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 43592\n",
      "  Number of trainable parameters = 124647170\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancheska_vicente\u001b[0m (\u001b[33mtonely\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230409_220256-7lfjcgg7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/7lfjcgg7' target=\"_blank\">zany-pyramid-119</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/7lfjcgg7' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/7lfjcgg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43592' max='43592' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43592/43592 6:29:33, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.134800</td>\n",
       "      <td>1.785331</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.991400</td>\n",
       "      <td>0.825983</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.830500</td>\n",
       "      <td>0.776718</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.703000</td>\n",
       "      <td>0.695388</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_28348\\730230934.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  precision_metric = load_metric(\"precision\")\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to roberta_trainer\\run-0\\checkpoint-20000\n",
      "Configuration saved in roberta_trainer\\run-0\\checkpoint-20000\\config.json\n",
      "Model weights saved in roberta_trainer\\run-0\\checkpoint-20000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to roberta_trainer\\run-0\\checkpoint-40000\n",
      "Configuration saved in roberta_trainer\\run-0\\checkpoint-40000\\config.json\n",
      "Model weights saved in roberta_trainer\\run-0\\checkpoint-40000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-04-10 04:32:41,916]\u001b[0m Trial 0 finished with value: 2.3315035877247845 and parameters: {'learning_rate': 0.01, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}. Best is trial 0 with value: 2.3315035877247845.\u001b[0m\n",
      "Trial: {'learning_rate': 0.001, 'per_device_train_batch_size': 8, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 43590\n",
      "  Number of trainable parameters = 124647170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48355d7bf994265b9aa62741e9e579a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Accuracy</td><td>‚ñà‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/F1 Macro Score</td><td>‚ñà‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/Precision</td><td>‚ñÅ‚ñà‚ñÅ‚ñà</td></tr><tr><td>eval/Recall</td><td>‚ñÅ‚ñà‚ñÅ‚ñà</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñà‚ñá‚ñÖ‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñà</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ‚ñÇ‚ñÑ‚ñà</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Accuracy</td><td>0.49925</td></tr><tr><td>eval/F1 Macro Score</td><td>0.333</td></tr><tr><td>eval/Precision</td><td>0.49925</td></tr><tr><td>eval/Recall</td><td>1.0</td></tr><tr><td>eval/loss</td><td>0.69539</td></tr><tr><td>eval/runtime</td><td>213.3167</td></tr><tr><td>eval/samples_per_second</td><td>90.818</td></tr><tr><td>eval/steps_per_second</td><td>11.354</td></tr><tr><td>train/epoch</td><td>4.0</td></tr><tr><td>train/global_step</td><td>43592</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.703</td></tr><tr><td>train/total_flos</td><td>1.834989122291712e+17</td></tr><tr><td>train/train_loss</td><td>0.9723</td></tr><tr><td>train/train_runtime</td><td>23387.1075</td></tr><tr><td>train/train_samples_per_second</td><td>29.821</td></tr><tr><td>train/train_steps_per_second</td><td>1.864</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zany-pyramid-119</strong> at: <a href='https://wandb.ai/tonely/huggingface/runs/7lfjcgg7' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/7lfjcgg7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230409_220256-7lfjcgg7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce42ce7dd4e34d5b8862191058fd24a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01718333333337796, max=1.0)‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230410_043247-0zd2o341</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/0zd2o341' target=\"_blank\">fast-plant-120</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/0zd2o341' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/0zd2o341</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='43590' max='43590' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [43590/43590 3:45:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.695200</td>\n",
       "      <td>0.693392</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.583600</td>\n",
       "      <td>1.026052</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to roberta_trainer\\run-1\\checkpoint-20000\n",
      "Configuration saved in roberta_trainer\\run-1\\checkpoint-20000\\config.json\n",
      "Model weights saved in roberta_trainer\\run-1\\checkpoint-20000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to roberta_trainer\\run-1\\checkpoint-40000\n",
      "Configuration saved in roberta_trainer\\run-1\\checkpoint-40000\\config.json\n",
      "Model weights saved in roberta_trainer\\run-1\\checkpoint-40000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-04-10 08:18:07,732]\u001b[0m Trial 1 finished with value: 0.8344142826144729 and parameters: {'learning_rate': 0.001, 'per_device_train_batch_size': 8, 'num_train_epochs': 2}. Best is trial 0 with value: 2.3315035877247845.\u001b[0m\n",
      "Trial: {'learning_rate': 0.01, 'per_device_train_batch_size': 16, 'num_train_epochs': 2}\n",
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 21796\n",
      "  Number of trainable parameters = 124647170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1600063f651404889e6a2d6af0350be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Accuracy</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/F1 Macro Score</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/Precision</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/Recall</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Accuracy</td><td>0.50075</td></tr><tr><td>eval/F1 Macro Score</td><td>0.33367</td></tr><tr><td>eval/Precision</td><td>0.0</td></tr><tr><td>eval/Recall</td><td>0.0</td></tr><tr><td>eval/loss</td><td>1.02605</td></tr><tr><td>eval/runtime</td><td>221.8741</td></tr><tr><td>eval/samples_per_second</td><td>87.315</td></tr><tr><td>eval/steps_per_second</td><td>10.916</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>43590</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5836</td></tr><tr><td>train/total_flos</td><td>9.17494561145856e+16</td></tr><tr><td>train/train_loss</td><td>0.66726</td></tr><tr><td>train/train_runtime</td><td>13524.245</td></tr><tr><td>train/train_samples_per_second</td><td>25.784</td></tr><tr><td>train/train_steps_per_second</td><td>3.223</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fast-plant-120</strong> at: <a href='https://wandb.ai/tonely/huggingface/runs/0zd2o341' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/0zd2o341</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230410_043247-0zd2o341\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a448884818c341fba2d70de868e94498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230410_081814-1l13hjfx</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/1l13hjfx' target=\"_blank\">zany-pyramid-121</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/1l13hjfx' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/1l13hjfx</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21796' max='21796' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21796/21796 3:17:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.012600</td>\n",
       "      <td>0.735783</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.718100</td>\n",
       "      <td>0.693415</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to roberta_trainer\\run-2\\checkpoint-20000\n",
      "Configuration saved in roberta_trainer\\run-2\\checkpoint-20000\\config.json\n",
      "Model weights saved in roberta_trainer\\run-2\\checkpoint-20000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-04-10 11:35:47,593]\u001b[0m Trial 2 finished with value: 2.3315035877247845 and parameters: {'learning_rate': 0.01, 'per_device_train_batch_size': 16, 'num_train_epochs': 2}. Best is trial 0 with value: 2.3315035877247845.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "best_trial_roberta = trainer_tuning.hyperparameter_search(\n",
    "    direction = \"maximize\",\n",
    "    backend = \"optuna\",\n",
    "    hp_space = optuna_hp_space,\n",
    "    n_trials = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b925b7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='0', objective=2.3315035877247845, hyperparameters={'learning_rate': 0.01, 'per_device_train_batch_size': 16, 'num_train_epochs': 4})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial_roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d1ee67",
   "metadata": {},
   "source": [
    "In the tuning, three RoBERTa models were created and compared with the following hyperparameter values:\n",
    "* **Learning Rate** = 0.01, **Train Batch Size** = 16, **Number of Train Epochs** = 4\n",
    "* **Learning Rate** = 0.001, **Train Batch Size** = 8, **Number of Train Epochs** = 2\n",
    "* **Learning Rate** = 0.01, **Train Batch Size** = 16, **Number of Train Epochs** = 2\n",
    "\n",
    "Out of these three, the best run for the RoBERTa model was the first model that **trained for four (4) epochs with the learning rate of 0.01 and the train batch size of 16**. However, based on the performance on the validation set, we can see that the RoBERTa base still performed better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d6063",
   "metadata": {},
   "source": [
    "##### Saving RoBERTa tuned model\n",
    "\n",
    "To use this model outside of this notebook, we will save the RoBERTa [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) object and the [`RoBERTa Tokenizer`](https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaTokenizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "def36697",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_models/RoBERTav2_tuned\n",
      "Configuration saved in ./saved_models/RoBERTav2_tuned\\config.json\n",
      "Model weights saved in ./saved_models/RoBERTav2_tuned\\pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/RoBERTav2_tuned\\tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/RoBERTav2_tuned\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_models/RoBERTav2_tuned\\\\tokenizer_config.json',\n",
       " './saved_models/RoBERTav2_tuned\\\\special_tokens_map.json',\n",
       " './saved_models/RoBERTav2_tuned\\\\vocab.json',\n",
       " './saved_models/RoBERTav2_tuned\\\\merges.txt',\n",
       " './saved_models/RoBERTav2_tuned\\\\added_tokens.json',\n",
       " './saved_models/RoBERTav2_tuned\\\\tokenizer.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_for_models ='./saved_models/RoBERTav2_tuned'\n",
    "trainer_tuning.save_model(path_for_models)\n",
    "roberta_tokenizer.save_pretrained(path_for_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec933140",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Last, let us see how the best model from the RoBERTa tuning fared in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a54749fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 48432\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6054' max='6054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6054/6054 08:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6934160590171814,\n",
       " 'eval_Accuracy': 0.49923604228609186,\n",
       " 'eval_F1 Macro Score': 0.33299362355565965,\n",
       " 'eval_Precision': 0.49923604228609186,\n",
       " 'eval_Recall': 1.0,\n",
       " 'eval_runtime': 535.8659,\n",
       " 'eval_samples_per_second': 90.381,\n",
       " 'eval_steps_per_second': 11.298,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_tuning.evaluate(eval_dataset = roberta_encoded_dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f279a843",
   "metadata": {},
   "source": [
    "From this, it is evident that the RoBERTa base performed better even in the test set compared to the model returned in the tuning.\n",
    "\n",
    "In conclusion, comparing the final models of the BERT and RoBERTa (i.e., which made use of the default values for their hyperparameters and the MAX_LENGTH of 512), the RoBERTa received a higher score for all of the metrics except for Recall. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
