{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0e70ac4",
   "metadata": {},
   "source": [
    "# **Insert Title Here**\n",
    "**DATA103 S11 Group 4**\n",
    "- GOZON, Jean Pauline D.\n",
    "- JAMIAS, Gillian Nicole A.\n",
    "- MARCELO Andrea Jean C. \n",
    "- REYES, Anton Gabriel G.\n",
    "- VICENTE, Francheska Josefa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee8a36f",
   "metadata": {},
   "source": [
    "## Requirements and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02257198",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87d6f11",
   "metadata": {},
   "source": [
    "**Basic Libraries**\n",
    "\n",
    "* `numpy` contains a large collection of mathematical functions\n",
    "* `pandas` contains functions that are designed for data manipulation and data analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b421a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ab390a",
   "metadata": {},
   "source": [
    "**Machine Learning Libraries**\n",
    "\n",
    "* `torch` this is an open source ML library for deep neural network creation\n",
    "* `transformers` contains pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99272409",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4873b00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_lightning.callbacks import ProgressBarBase, RichProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7a2e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainerCallback, TrainingArguments, Trainer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9eee55fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, hamming_loss, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import evaluate\n",
    "\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c42ff241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e848d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7300021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['Its not a viable option, and youll be leavin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['It can be hard to appreciate the notion that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>['Hi, so last night i was sitting on the ledge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>['I tried to kill my self once and failed badl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>['Hi NEM3030. What sorts of things do you enjo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242155</th>\n",
       "      <td>0</td>\n",
       "      <td>If you don't like rock then your not going to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242156</th>\n",
       "      <td>0</td>\n",
       "      <td>You how you can tell i have so many friends an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242157</th>\n",
       "      <td>0</td>\n",
       "      <td>pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242158</th>\n",
       "      <td>1</td>\n",
       "      <td>The usual stuff you find hereI'm not posting t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242159</th>\n",
       "      <td>0</td>\n",
       "      <td>I still haven't beaten the first boss in Hollo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>242160 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        class                                               text\n",
       "0           0  ['Its not a viable option, and youll be leavin...\n",
       "1           1  ['It can be hard to appreciate the notion that...\n",
       "2           1  ['Hi, so last night i was sitting on the ledge...\n",
       "3           1  ['I tried to kill my self once and failed badl...\n",
       "4           1  ['Hi NEM3030. What sorts of things do you enjo...\n",
       "...       ...                                                ...\n",
       "242155      0  If you don't like rock then your not going to ...\n",
       "242156      0  You how you can tell i have so many friends an...\n",
       "242157      0  pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...\n",
       "242158      1  The usual stuff you find hereI'm not posting t...\n",
       "242159      0  I still haven't beaten the first boss in Hollo...\n",
       "\n",
       "[242160 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv ('cleaned_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a26a1ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bc7c89",
   "metadata": {},
   "source": [
    "## Preparing data for Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea6faa1",
   "metadata": {},
   "source": [
    "### Splitting the Dataset into Train, Val, and Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef2d8078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         ['Its not a viable option, and youll be leavin...\n",
       "1         ['It can be hard to appreciate the notion that...\n",
       "2         ['Hi, so last night i was sitting on the ledge...\n",
       "3         ['I tried to kill my self once and failed badl...\n",
       "4         ['Hi NEM3030. What sorts of things do you enjo...\n",
       "                                ...                        \n",
       "242155    If you don't like rock then your not going to ...\n",
       "242156    You how you can tell i have so many friends an...\n",
       "242157    pee probably tastes like salty teaüòèüí¶‚ÄºÔ∏è can som...\n",
       "242158    The usual stuff you find hereI'm not posting t...\n",
       "242159    I still haven't beaten the first boss in Hollo...\n",
       "Name: text, Length: 242160, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df ['text']\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83e11442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         1\n",
       "         ..\n",
       "242155    0\n",
       "242156    0\n",
       "242157    0\n",
       "242158    1\n",
       "242159    0\n",
       "Name: class, Length: 242160, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df ['class']\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "314aab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2,\n",
    "                                                    stratify = y,\n",
    "                                                    random_state = 42, \n",
    "                                                    shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aeb7acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                  y_train, \n",
    "                                                  test_size = 0.1,\n",
    "                                                  stratify = y_train,\n",
    "                                                  random_state = 42, \n",
    "                                                  shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2b8deec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input  shape:  (174355,)\n",
      "Train output shape:  (174355,)\n"
     ]
    }
   ],
   "source": [
    "print('Train input  shape: ', X_train.shape)\n",
    "print('Train output shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "803c0901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val input  shape:  (19373,)\n",
      "Val output shape:  (19373,)\n"
     ]
    }
   ],
   "source": [
    "print('Val input  shape: ', X_val.shape)\n",
    "print('Val output shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00650cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test input  shape:  (48432,)\n",
      "Test output shape:  (48432,)\n"
     ]
    }
   ],
   "source": [
    "print('Test input  shape: ', X_test.shape)\n",
    "print('Test output shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8cd9f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How do you explain to your family that you wer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I DONT UNDERSTAND THE US DEBT WHO DO THEY OWE ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FireIt‚Äôs been a bit but I still think of her a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AITA for telling my wife (34F) that reddit agr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Join among us SGGFIF Jesjeuejjejejeeieieijdjdj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174350</th>\n",
       "      <td>Fellow teenagers, I have been influenced by th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174351</th>\n",
       "      <td>I felt like talkingSo I was just outside at 01...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174352</th>\n",
       "      <td>i am trying to but i just cant i have everythi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174353</th>\n",
       "      <td>I just want my suffering to endAll I have hear...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174354</th>\n",
       "      <td>How can you stand the pain?While I was student...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>174355 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  class\n",
       "0       How do you explain to your family that you wer...      0\n",
       "1       I DONT UNDERSTAND THE US DEBT WHO DO THEY OWE ...      0\n",
       "2       FireIt‚Äôs been a bit but I still think of her a...      1\n",
       "3       AITA for telling my wife (34F) that reddit agr...      0\n",
       "4       Join among us SGGFIF Jesjeuejjejejeeieieijdjdj...      0\n",
       "...                                                   ...    ...\n",
       "174350  Fellow teenagers, I have been influenced by th...      0\n",
       "174351  I felt like talkingSo I was just outside at 01...      1\n",
       "174352  i am trying to but i just cant i have everythi...      1\n",
       "174353  I just want my suffering to endAll I have hear...      1\n",
       "174354  How can you stand the pain?While I was student...      1\n",
       "\n",
       "[174355 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.concat([X_train, y_train], axis = 1).reset_index(drop = True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "437f17b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Really down........just need some words of enc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I‚Äôm not gonna buy a carThe day gets closer. I‚Äô...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Help me kill myself. Please. Please. Please.I‚Äô...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The only thing keeping me alive is the fact th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm not.I'm not the sweet, determined girl eve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19368</th>\n",
       "      <td>when she says Hi! This post seems to be relate...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19369</th>\n",
       "      <td>I gotta go to school tmmr for orientation at 9...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19370</th>\n",
       "      <td>Hey lads! Can I get some help from y'all? So.....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19371</th>\n",
       "      <td>My birthday is this coming month and it will b...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19372</th>\n",
       "      <td>Posting songs I like day 20 https://youtu.be/i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19373 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  class\n",
       "0      Really down........just need some words of enc...      1\n",
       "1      I‚Äôm not gonna buy a carThe day gets closer. I‚Äô...      1\n",
       "2      Help me kill myself. Please. Please. Please.I‚Äô...      1\n",
       "3      The only thing keeping me alive is the fact th...      1\n",
       "4      I'm not.I'm not the sweet, determined girl eve...      1\n",
       "...                                                  ...    ...\n",
       "19368  when she says Hi! This post seems to be relate...      0\n",
       "19369  I gotta go to school tmmr for orientation at 9...      0\n",
       "19370  Hey lads! Can I get some help from y'all? So.....      0\n",
       "19371  My birthday is this coming month and it will b...      1\n",
       "19372  Posting songs I like day 20 https://youtu.be/i...      0\n",
       "\n",
       "[19373 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df = pd.concat([X_val, y_val], axis = 1).reset_index(drop = True)\n",
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cbabe1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I just felt myself snapI have to pretend to be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Are you envious of something about the opposit...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We get it. Men have problems, too. We never sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Happy Birthday to everyone having Birthday on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i cant deal with life any longer but ive tried...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48427</th>\n",
       "      <td>I just need to go for everyone's sakeI can't e...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48428</th>\n",
       "      <td>Hope is now goneI'm 17m and I'm considering ta...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48429</th>\n",
       "      <td>18f needs someone to talk toI understand if th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48430</th>\n",
       "      <td>Help mePlease someone help me, just pm me.\\nI'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48431</th>\n",
       "      <td>bf application üòé yo. i‚Äôm 17 i‚Äôm a gorl. and th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48432 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  class\n",
       "0      I just felt myself snapI have to pretend to be...      1\n",
       "1      Are you envious of something about the opposit...      0\n",
       "2      We get it. Men have problems, too. We never sa...      0\n",
       "3      Happy Birthday to everyone having Birthday on ...      0\n",
       "4      i cant deal with life any longer but ive tried...      1\n",
       "...                                                  ...    ...\n",
       "48427  I just need to go for everyone's sakeI can't e...      1\n",
       "48428  Hope is now goneI'm 17m and I'm considering ta...      1\n",
       "48429  18f needs someone to talk toI understand if th...      1\n",
       "48430  Help mePlease someone help me, just pm me.\\nI'...      1\n",
       "48431  bf application üòé yo. i‚Äôm 17 i‚Äôm a gorl. and th...      0\n",
       "\n",
       "[48432 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.concat([X_test, y_test], axis = 1).reset_index(drop = True)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61a052",
   "metadata": {},
   "source": [
    "### Creation of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c555f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'class'],\n",
       "    num_rows: 174355\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = datasets.Dataset.from_pandas(train_df)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3092821e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'class'],\n",
       "    num_rows: 19373\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset = datasets.Dataset.from_pandas(val_df)\n",
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ac0956b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'class'],\n",
       "    num_rows: 48432\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = datasets.Dataset.from_pandas(test_df)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "673bbef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'class'],\n",
       "        num_rows: 174355\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'class'],\n",
       "        num_rows: 19373\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'class'],\n",
       "        num_rows: 48432\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.DatasetDict({\n",
    "    \"train\" : train_dataset, \n",
    "    \"val\" : val_dataset, \n",
    "    \"test\" : test_dataset\n",
    "})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c95903",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80bccf6",
   "metadata": {},
   "source": [
    "### Defining of Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d9d4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fca978aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, tokenizer):\n",
    "    encoding = tokenizer(examples[\"text\"], padding = \"max_length\", truncation = True, max_length = MAX_LENGTH)\n",
    "    encoding[\"labels\"] = torch.tensor(examples ['class'])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82755943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoded_dataset (tokenizer):\n",
    "    encoded_dataset = dataset.map(preprocess_function, \n",
    "                                  batched=True, \n",
    "                                  remove_columns=dataset['train'].column_names, \n",
    "                                  fn_kwargs = {\"tokenizer\": tokenizer})\n",
    "    \n",
    "    encoded_dataset.set_format(\"torch\")\n",
    "    \n",
    "    return encoded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900fffaf",
   "metadata": {},
   "source": [
    "### Tokenizing with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48eeead9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', use_fast = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cba846e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1050bf338ce4dc0ad9fe301796af80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da7378a6b3f49ccaeb2c7f9603777ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d70a17d23c43049d2f4e60110830d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bert_encoded_dataset = create_encoded_dataset (bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787036ae",
   "metadata": {},
   "source": [
    "### Tokenizing with RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ace6ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_roberta = 'roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1c7e5f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = AutoTokenizer.from_pretrained(model_checkpoint_roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab5b42a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c88602f12a54816bd5b6f05ced01d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/175 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5f5bd5678a45bc83f59dfca53e4273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53fc4081ecf4fe5ba1c2ac076ebeffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/49 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "roberta_encoded_dataset = create_encoded_dataset (roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779da9d",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756f4719",
   "metadata": {},
   "source": [
    "### Defining of Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c6a9b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: EvalPrediction):\n",
    "    logits, labels = p\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    precision_metric = load_metric(\"precision\")\n",
    "    recall_metric = load_metric(\"recall\")\n",
    "    accuracy_metric = load_metric(\"accuracy\")\n",
    "    f1_metric = load_metric(\"f1\")\n",
    "    \n",
    "    f1_macro_score = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    accuracy_score = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision_score = precision_metric.compute(predictions=predictions, references=labels)\n",
    "    recall_score = recall_metric.compute(predictions=predictions, references=labels)\n",
    "    \n",
    "    results = {\n",
    "        'Accuracy' : accuracy_score ['accuracy'],\n",
    "        'F1 Macro Score' : f1_macro_score ['f1'], \n",
    "        'Precision' : precision_score[\"precision\"],\n",
    "        'Recall' : recall_score[\"recall\"]\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4d926a",
   "metadata": {},
   "source": [
    "### Defining of Hyperparameter Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50a5cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [0.1, 0.01, 0.001]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16]),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [2, 3, 4])\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bb8827",
   "metadata": {},
   "source": [
    "### BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841056d",
   "metadata": {},
   "source": [
    "#### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "49f8a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7dcd65db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels = 2, \n",
    "    max_length = MAX_LENGTH\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b5940d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = \"bert_trainer\", \n",
    "                                  save_steps = 20000,\n",
    "                                  save_strategy = 'steps',\n",
    "                                  fp16 = True,\n",
    "                                  evaluation_strategy = \"epoch\", \n",
    "                                  resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6ba97d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = bert_model,\n",
    "    args = training_args,\n",
    "    train_dataset = bert_encoded_dataset ['train'],\n",
    "    eval_dataset = bert_encoded_dataset ['val'],\n",
    "    tokenizer = bert_tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [TrainerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43cb5bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65385\n",
      "  Number of trainable parameters = 108311810\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancheska_vicente\u001b[0m (\u001b[33mtonely\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230406_205221-7esay8x9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/7esay8x9' target=\"_blank\">changeling-blood-wine-109</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/7esay8x9' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/7esay8x9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65385' max='65385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65385/65385 5:33:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.367400</td>\n",
       "      <td>0.413518</td>\n",
       "      <td>0.882930</td>\n",
       "      <td>0.882769</td>\n",
       "      <td>0.855347</td>\n",
       "      <td>0.921319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.561000</td>\n",
       "      <td>1.071023</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.194600</td>\n",
       "      <td>0.162171</td>\n",
       "      <td>0.952924</td>\n",
       "      <td>0.952922</td>\n",
       "      <td>0.957012</td>\n",
       "      <td>0.948304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert_trainer\\checkpoint-20000\n",
      "Configuration saved in bert_trainer\\checkpoint-20000\\config.json\n",
      "Model weights saved in bert_trainer\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\checkpoint-20000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_8112\\1346387678.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  precision_metric = load_metric(\"precision\")\n",
      "Saving model checkpoint to bert_trainer\\checkpoint-40000\n",
      "Configuration saved in bert_trainer\\checkpoint-40000\\config.json\n",
      "Model weights saved in bert_trainer\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\checkpoint-40000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert_trainer\\checkpoint-60000\n",
      "Configuration saved in bert_trainer\\checkpoint-60000\\config.json\n",
      "Model weights saved in bert_trainer\\checkpoint-60000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\checkpoint-60000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\checkpoint-60000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=65385, training_loss=0.3582221678567332, metrics={'train_runtime': 20003.5531, 'train_samples_per_second': 26.149, 'train_steps_per_second': 3.269, 'total_flos': 1.376241841718784e+17, 'train_loss': 0.3582221678567332, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e5de1c",
   "metadata": {},
   "source": [
    "#### Saving BERT base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c84dd6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_for_models ='./saved_models/BERTv4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "331f536c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_models/BERTv4\n",
      "Configuration saved in ./saved_models/BERTv4\\config.json\n",
      "Model weights saved in ./saved_models/BERTv4\\pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/BERTv4\\tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/BERTv4\\special_tokens_map.json\n",
      "tokenizer config file saved in ./saved_models/BERTv4\\tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/BERTv4\\special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./saved_models/BERTv4\\\\tokenizer_config.json',\n",
       " './saved_models/BERTv4\\\\special_tokens_map.json',\n",
       " './saved_models/BERTv4\\\\vocab.txt',\n",
       " './saved_models/BERTv4\\\\added_tokens.json')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(path_for_models)\n",
    "bert_tokenizer.save_pretrained(path_for_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aa0e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(eval_dataset=bert_encoded_dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb64f4",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbb450f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint,\n",
    "                                                              num_labels = 2, \n",
    "                                                              max_length = MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7530d21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_tuning = TrainingArguments(output_dir = \"bert_trainer\", \n",
    "                                         save_steps = 20000, \n",
    "                                         bf16 = True,\n",
    "                                         save_strategy = 'steps',\n",
    "                                         evaluation_strategy = \"epoch\", \n",
    "                                         resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b3e177b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer_tuning = Trainer(\n",
    "    model_init = model_init,\n",
    "    args = training_args_tuning,\n",
    "    train_dataset = bert_encoded_dataset ['train'],\n",
    "    eval_dataset = bert_encoded_dataset ['val'],\n",
    "    tokenizer = bert_tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [TrainerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3358c16f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-07 06:36:09,875]\u001b[0m A new study created in memory with name: no-name-8501078e-7df7-41fd-b022-6dc74c71cc6e\u001b[0m\n",
      "Trial: {'learning_rate': 0.001, 'per_device_train_batch_size': 8, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65385\n",
      "  Number of trainable parameters = 108311810\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancheska_vicente\u001b[0m (\u001b[33mtonely\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230407_063613-7ncnmqqu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/7ncnmqqu' target=\"_blank\">nemesis-unimatrix-110</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/7ncnmqqu' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/7ncnmqqu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65385' max='65385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65385/65385 5:33:36, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.751700</td>\n",
       "      <td>0.710669</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.720200</td>\n",
       "      <td>0.695365</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.697200</td>\n",
       "      <td>0.691395</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert_trainer\\run-0\\checkpoint-20000\n",
      "Configuration saved in bert_trainer\\run-0\\checkpoint-20000\\config.json\n",
      "Model weights saved in bert_trainer\\run-0\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-0\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-0\\checkpoint-20000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20460\\730230934.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  precision_metric = load_metric(\"precision\")\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to bert_trainer\\run-0\\checkpoint-40000\n",
      "Configuration saved in bert_trainer\\run-0\\checkpoint-40000\\config.json\n",
      "Model weights saved in bert_trainer\\run-0\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-0\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-0\\checkpoint-40000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert_trainer\\run-0\\checkpoint-60000\n",
      "Configuration saved in bert_trainer\\run-0\\checkpoint-60000\\config.json\n",
      "Model weights saved in bert_trainer\\run-0\\checkpoint-60000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-0\\checkpoint-60000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-0\\checkpoint-60000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-04-07 12:10:01,271]\u001b[0m Trial 0 finished with value: 0.8344142826144729 and parameters: {'learning_rate': 0.001, 'per_device_train_batch_size': 8, 'num_train_epochs': 3}. Best is trial 0 with value: 0.8344142826144729.\u001b[0m\n",
      "Trial: {'learning_rate': 0.1, 'per_device_train_batch_size': 8, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65385\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f24de99e4d84c7ba599028b6adf8acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.041 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.033370‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Accuracy</td><td>‚ñà‚ñÅ‚ñà</td></tr><tr><td>eval/F1 Macro Score</td><td>‚ñà‚ñÅ‚ñà</td></tr><tr><td>eval/Precision</td><td>‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/Recall</td><td>‚ñÅ‚ñà‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÇ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ‚ñÖ‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñà‚ñÑ‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñà‚ñÑ‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Accuracy</td><td>0.50075</td></tr><tr><td>eval/F1 Macro Score</td><td>0.33367</td></tr><tr><td>eval/Precision</td><td>0.0</td></tr><tr><td>eval/Recall</td><td>0.0</td></tr><tr><td>eval/loss</td><td>0.69139</td></tr><tr><td>eval/runtime</td><td>221.2597</td></tr><tr><td>eval/samples_per_second</td><td>87.558</td></tr><tr><td>eval/steps_per_second</td><td>10.946</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>65385</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.6972</td></tr><tr><td>train/total_flos</td><td>1.376241841718784e+17</td></tr><tr><td>train/train_loss</td><td>0.73432</td></tr><tr><td>train/train_runtime</td><td>20030.0377</td></tr><tr><td>train/train_samples_per_second</td><td>26.114</td></tr><tr><td>train/train_steps_per_second</td><td>3.264</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nemesis-unimatrix-110</strong> at: <a href='https://wandb.ai/tonely/huggingface/runs/7ncnmqqu' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/7ncnmqqu</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230407_063613-7ncnmqqu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d50ba7479b3747e6999806a1f23a9a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666414434, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230407_121008-bv89f701</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/bv89f701' target=\"_blank\">frosty-terrain-111</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/bv89f701' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/bv89f701</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65385' max='65385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65385/65385 5:34:02, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.033400</td>\n",
       "      <td>11.454621</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.321500</td>\n",
       "      <td>4.412920</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.805800</td>\n",
       "      <td>0.695219</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert_trainer\\run-1\\checkpoint-20000\n",
      "Configuration saved in bert_trainer\\run-1\\checkpoint-20000\\config.json\n",
      "Model weights saved in bert_trainer\\run-1\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-1\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-1\\checkpoint-20000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert_trainer\\run-1\\checkpoint-40000\n",
      "Configuration saved in bert_trainer\\run-1\\checkpoint-40000\\config.json\n",
      "Model weights saved in bert_trainer\\run-1\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-1\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-1\\checkpoint-40000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert_trainer\\run-1\\checkpoint-60000\n",
      "Configuration saved in bert_trainer\\run-1\\checkpoint-60000\\config.json\n",
      "Model weights saved in bert_trainer\\run-1\\checkpoint-60000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-1\\checkpoint-60000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-1\\checkpoint-60000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-04-07 17:44:21,185]\u001b[0m Trial 1 finished with value: 0.8344142826144729 and parameters: {'learning_rate': 0.1, 'per_device_train_batch_size': 8, 'num_train_epochs': 3}. Best is trial 0 with value: 0.8344142826144729.\u001b[0m\n",
      "Trial: {'learning_rate': 0.1, 'per_device_train_batch_size': 8, 'num_train_epochs': 3}\n",
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--bert-base-cased\\snapshots\\5532cc56f74641d4bb33641f5c76a55d11f846e0\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65385\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe03299fcbdc4f2d939bc053b5cd343e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Accuracy</td><td>‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>eval/F1 Macro Score</td><td>‚ñÅ‚ñÅ‚ñà</td></tr><tr><td>eval/Precision</td><td>‚ñà‚ñà‚ñÅ</td></tr><tr><td>eval/Recall</td><td>‚ñà‚ñà‚ñÅ</td></tr><tr><td>eval/loss</td><td>‚ñà‚ñÉ‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÖ‚ñÅ‚ñà</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÑ‚ñà‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÑ‚ñà‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/learning_rate</td><td>‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñá‚ñà‚ñá‚ñà‚ñÜ‚ñá‚ñá‚ñá‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Accuracy</td><td>0.50075</td></tr><tr><td>eval/F1 Macro Score</td><td>0.33367</td></tr><tr><td>eval/Precision</td><td>0.0</td></tr><tr><td>eval/Recall</td><td>0.0</td></tr><tr><td>eval/loss</td><td>0.69522</td></tr><tr><td>eval/runtime</td><td>219.0251</td></tr><tr><td>eval/samples_per_second</td><td>88.451</td></tr><tr><td>eval/steps_per_second</td><td>11.058</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>65385</td></tr><tr><td>train/learning_rate</td><td>0.00059</td></tr><tr><td>train/loss</td><td>0.8058</td></tr><tr><td>train/total_flos</td><td>1.376241841718784e+17</td></tr><tr><td>train/train_loss</td><td>5.76942</td></tr><tr><td>train/train_runtime</td><td>20058.3406</td></tr><tr><td>train/train_samples_per_second</td><td>26.077</td></tr><tr><td>train/train_steps_per_second</td><td>3.26</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frosty-terrain-111</strong> at: <a href='https://wandb.ai/tonely/huggingface/runs/bv89f701' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/bv89f701</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230407_121008-bv89f701\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30c8924bce4342f4be68bac82406b923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230407_174428-34malvo9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/34malvo9' target=\"_blank\">elated-resonance-112</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/34malvo9' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/34malvo9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65385' max='65385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65385/65385 5:33:15, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.033400</td>\n",
       "      <td>11.454621</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.321500</td>\n",
       "      <td>4.412920</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>0.333001</td>\n",
       "      <td>0.499252</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.805800</td>\n",
       "      <td>0.695219</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bert_trainer\\run-2\\checkpoint-20000\n",
      "Configuration saved in bert_trainer\\run-2\\checkpoint-20000\\config.json\n",
      "Model weights saved in bert_trainer\\run-2\\checkpoint-20000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-2\\checkpoint-20000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-2\\checkpoint-20000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert_trainer\\run-2\\checkpoint-40000\n",
      "Configuration saved in bert_trainer\\run-2\\checkpoint-40000\\config.json\n",
      "Model weights saved in bert_trainer\\run-2\\checkpoint-40000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-2\\checkpoint-40000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-2\\checkpoint-40000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to bert_trainer\\run-2\\checkpoint-60000\n",
      "Configuration saved in bert_trainer\\run-2\\checkpoint-60000\\config.json\n",
      "Model weights saved in bert_trainer\\run-2\\checkpoint-60000\\pytorch_model.bin\n",
      "tokenizer config file saved in bert_trainer\\run-2\\checkpoint-60000\\tokenizer_config.json\n",
      "Special tokens file saved in bert_trainer\\run-2\\checkpoint-60000\\special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001b[32m[I 2023-04-07 23:17:54,665]\u001b[0m Trial 2 finished with value: 0.8344142826144729 and parameters: {'learning_rate': 0.1, 'per_device_train_batch_size': 8, 'num_train_epochs': 3}. Best is trial 0 with value: 0.8344142826144729.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "best_trial = trainer_tuning.hyperparameter_search(\n",
    "    direction = \"maximize\",\n",
    "    backend = \"optuna\",\n",
    "    hp_space = optuna_hp_space,\n",
    "    n_trials = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fbd70eaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='0', objective=0.8344142826144729, hyperparameters={'learning_rate': 0.001, 'per_device_train_batch_size': 8, 'num_train_epochs': 3})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda5e1e3",
   "metadata": {},
   "source": [
    "##### Saving BERT tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7ab72ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_models/BERTv2_tuned\n",
      "Configuration saved in ./saved_models/BERTv2_tuned\\config.json\n",
      "Model weights saved in ./saved_models/BERTv2_tuned\\pytorch_model.bin\n",
      "tokenizer config file saved in ./saved_models/BERTv2_tuned\\tokenizer_config.json\n",
      "Special tokens file saved in ./saved_models/BERTv2_tuned\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "path_for_models ='./saved_models/BERTv2_tuned'\n",
    "trainer_tuning.save_model(path_for_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc78564",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1379f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_tuning.evaluate(eval_dataset=bert_encoded_dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e25b08",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a6185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fe26554",
   "metadata": {},
   "source": [
    "### RoBERTa Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5a119a",
   "metadata": {},
   "source": [
    "#### Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "55dfa1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_roberta = 'roberta-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd78e4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "roberta_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint_roberta,\n",
    "    num_labels = 2, \n",
    "    max_length = MAX_LENGTH\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d7d267e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(output_dir = \"roberta_trainer\", \n",
    "                                  save_steps = 20000,\n",
    "                                  save_strategy = 'steps',\n",
    "                                  fp16 = True,\n",
    "                                  evaluation_strategy = \"epoch\", \n",
    "                                  resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b12aca14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_model.config.max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71ce2c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model = roberta_model,\n",
    "    args = training_args,\n",
    "    train_dataset = roberta_encoded_dataset ['train'],\n",
    "    eval_dataset = roberta_encoded_dataset ['val'],\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [TrainerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0364213f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 65385\n",
      "  Number of trainable parameters = 124647170\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancheska_vicente\u001b[0m (\u001b[33mtonely\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\admin\\Desktop\\Uni\\DATA103\\project-repository\\data103-project\\wandb\\run-20230408_202820-9kktjdac</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tonely/huggingface/runs/9kktjdac' target=\"_blank\">rare-leaf-115</a></strong> to <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tonely/huggingface' target=\"_blank\">https://wandb.ai/tonely/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tonely/huggingface/runs/9kktjdac' target=\"_blank\">https://wandb.ai/tonely/huggingface/runs/9kktjdac</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65385' max='65385' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [65385/65385 5:43:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 macro score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.622600</td>\n",
       "      <td>1.136392</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.693500</td>\n",
       "      <td>0.934038</td>\n",
       "      <td>0.500748</td>\n",
       "      <td>0.333666</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.171400</td>\n",
       "      <td>0.176044</td>\n",
       "      <td>0.954731</td>\n",
       "      <td>0.954715</td>\n",
       "      <td>0.970673</td>\n",
       "      <td>0.937655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to roberta_trainer\\checkpoint-20000\n",
      "Configuration saved in roberta_trainer\\checkpoint-20000\\config.json\n",
      "Model weights saved in roberta_trainer\\checkpoint-20000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_3484\\730230934.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  precision_metric = load_metric(\"precision\")\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to roberta_trainer\\checkpoint-40000\n",
      "Configuration saved in roberta_trainer\\checkpoint-40000\\config.json\n",
      "Model weights saved in roberta_trainer\\checkpoint-40000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to roberta_trainer\\checkpoint-60000\n",
      "Configuration saved in roberta_trainer\\checkpoint-60000\\config.json\n",
      "Model weights saved in roberta_trainer\\checkpoint-60000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 19373\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=65385, training_loss=0.5497573370633435, metrics={'train_runtime': 20653.9299, 'train_samples_per_second': 25.325, 'train_steps_per_second': 3.166, 'total_flos': 1.376241841718784e+17, 'train_loss': 0.5497573370633435, 'epoch': 3.0})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f3224c",
   "metadata": {},
   "source": [
    "#### Saving RoBERTa base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ea34aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./saved_models/RoBERTav2\n",
      "Configuration saved in ./saved_models/RoBERTav2\\config.json\n",
      "Model weights saved in ./saved_models/RoBERTav2\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "path_for_models ='./saved_models/RoBERTav2'\n",
    "trainer.save_model(path_for_models)\n",
    "roberta_tokenizer.save_pretrained(path_for_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a3c5b0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 48432\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6054' max='6054' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6054/6054 08:37]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.16810336709022522,\n",
       " 'eval_Accuracy': 0.95701189296333,\n",
       " 'eval_F1 Macro Score': 0.9569997463552207,\n",
       " 'eval_Precision': 0.9713724988267417,\n",
       " 'eval_Recall': 0.9416435750031019,\n",
       " 'eval_runtime': 524.6124,\n",
       " 'eval_samples_per_second': 92.32,\n",
       " 'eval_steps_per_second': 11.54,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=roberta_encoded_dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2d71b6",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5adedfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init_roberta ():\n",
    "    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint_roberta,\n",
    "                                                              num_labels = 2, \n",
    "                                                              max_length = MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6b3fc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_tuning = TrainingArguments(output_dir = \"bert_trainer\", \n",
    "                                         save_steps = 20000, \n",
    "                                         bf16 = True,\n",
    "                                         save_strategy = 'steps',\n",
    "                                         evaluation_strategy = \"epoch\", \n",
    "                                         resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cb52b1ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer_tuning = Trainer(\n",
    "    model_init = model_init_roberta,\n",
    "    args = training_args_tuning,\n",
    "    train_dataset = roberta_encoded_dataset ['train'],\n",
    "    eval_dataset = roberta_encoded_dataset ['val'],\n",
    "    tokenizer = roberta_tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [TrainerCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f231c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-09 05:35:48,793]\u001b[0m A new study created in memory with name: no-name-0beb4556-dcee-4bc2-9c78-0e427a331125\u001b[0m\n",
      "Trial: {'learning_rate': 0.01, 'per_device_train_batch_size': 16, 'num_train_epochs': 4}\n",
      "loading configuration file config.json from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\admin/.cache\\huggingface\\hub\\models--roberta-base\\snapshots\\bc2764f8af2e92b6eb5679868df33e224075ca68\\pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 174355\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 43592\n",
      "  Number of trainable parameters = 124647170\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfrancheska_vicente\u001b[0m (\u001b[33mtonely\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "best_trial_roberta = trainer_tuning.hyperparameter_search(\n",
    "    direction = \"maximize\",\n",
    "    backend = \"optuna\",\n",
    "    hp_space = optuna_hp_space,\n",
    "    n_trials = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b925b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial_roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d6063",
   "metadata": {},
   "source": [
    "##### Saving RoBERTa tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def36697",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_for_models ='./saved_models/RoBERTav1_tuned'\n",
    "trainer_tuning.save_model(path_for_models)\n",
    "roberta_tokenizer.save_pretrained(path_for_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec933140",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54749fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_tuning.evaluate(eval_dataset = roberta_encoded_dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9970a79e",
   "metadata": {},
   "source": [
    "#### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbda4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
